{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2825a697-1f89-4c34-a0e8-a1ab6384c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thuva_siva29/green-software-foundation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-26 11:43:18.348714: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-26 11:43:18.806660: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-26 11:43:18.999962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-26 11:43:19.001364: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-26 11:43:19.334026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 11:43:20.672739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "import os\n",
    "from get_embedding_function import get_embedding_function\n",
    "from populate_database import setup_database\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "import pandas as pd\n",
    "from dspy.datasets.dataset import Dataset\n",
    "from dspy.evaluate.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9913efcc-d894-49ce-940f-91d8832c5ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Example({'question': 'Is there any mention of optimizing storage utilization to reduce energy consumption?', 'answer': 'Yes'}) (input_keys={'question'}), Example({'question': 'Is there any mention of adopting serverless cloud services to optimize resource usage?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of allowing independent scaling of logical components?', 'answer': 'Yes'}) (input_keys={'question'}), Example({'question': 'Is there any mention of scaling down Kubernetes applications when not in use to reduce resource usage?', 'answer': 'Not Applicable'}) (input_keys={'question'}), Example({'question': 'Is there any mention of reducing transmitted data to save energy?', 'answer': 'Yes'}) (input_keys={'question'}), Example({'question': 'Is there any mention of optimizing average CPU utilization to ensure efficient use of resources?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of managing peak CPU utilization to avoid over-provisioning?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of scaling infrastructure based on real-time user demand?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?', 'answer': 'Not Applicable'}) (input_keys={'question'}), Example({'question': 'Is there any mention of removing unused assets to optimize performance?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of minimizing the total number of deployed environments to save resources?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of setting storage retention policies to automatically clean up old data?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of scaling down applications during idle periods to minimize resource usage?', 'answer': 'No'}) (input_keys={'question'}), Example({'question': 'Is there any mention of queuing non-urgent processing requests to reduce peak loads?', 'answer': 'Yes'}) (input_keys={'question'})]\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = \"queries_judgements.csv\"\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, file_path, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        self._train = df.iloc[0:14].to_dict(orient='records')\n",
    "\n",
    "        self._dev = df.iloc[15:22].to_dict(orient='records')\n",
    "\n",
    "dataset = CSVDataset(TRAINING_DATA)\n",
    "\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "print(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e887f959-a738-4fdb-a381-55ccb2dc3c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨  Database Cleared\n",
      "Number of existing documents in DB: 0\n",
      "ðŸ‘‰ Adding new documents: 32\n"
     ]
    }
   ],
   "source": [
    "CHROMA_PATH = os.getenv(\"CHROMA_PATH\")\n",
    "DOCUMENT_PATH=\"./documents/4.pdf\"\n",
    "\n",
    "emb_local = False\n",
    "\n",
    "setup_database(DOCUMENT_PATH, True, emb_local, True)\n",
    "\n",
    "embedder, collection_name = get_embedding_function(run_local=emb_local)\n",
    "\n",
    "retriever_model = ChromadbRM(\n",
    "    collection_name,\n",
    "    CHROMA_PATH,\n",
    "    embedding_function=embedder.embed,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "lm = dspy.OllamaLocal(model='phi3')\n",
    "\n",
    "dspy.settings.configure(lm=lm, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f337eb58-63d4-45d3-b850-c42a9ed0bc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['these properties.\\n2. Capacity Estimation for Dropbox System Design\\nStorage Estimations:\\nAssumptions:', 'The total number of users = 500 million.\\nTotal number of daily active users = 100 million\\nThe average number of files stored by each user = 200\\nThe average size of each file = 100 KB\\nTotal number of active connections per minute = 1 million\\n\\nStorage Estimations:\\n\\nTotal number of files = 500 million * 200 = 100 billion\\nTotal storage required = 100 billion * 100 KB = 10 PB\\n\\n3. High-Level Design(HLD) of Dropbox System Design\\n\\n\\n-----\\n\\ncommunicates with the Upload Service on the server side. Large files may be broken into smaller chunks\\nfor efficient transfer.', '7. Scalabilty for Dropbox System Design\\nHorizontal Scaling\\nWe can add more servers behind the load balancer to increase the capacity of each service. This is\\nknown as Horizontal Scaling and each service can be independently scaled horizontally in our design.\\nDatabase Sharding\\nMetadata DB is sharded based on object_id. Our hash function will map each object_id to a random\\nserver where we can store the file/folder metadata. To query for a particular object_id, service can\\ndetermine the database server using same hash function and query for data. This approach will distribute\\nour database load to multiple servers making it scalable.\\nCache Sharding\\nSimilar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has out']\n"
     ]
    }
   ],
   "source": [
    "retriever = dspy.Retrieve(k=3)\n",
    "print(retriever(\"Is there any mention of optimizing the size of AI/ML models to save storage space and reduce memory usage during inference\").passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d50637-d017-46fc-b0f2-a5aaf90e6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Give a concrete 'yes' or 'no' or 'not applicable' answer to a question based on the retrieved context.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Output nothing else apart from either 'Yes', 'No', or Not Applicable.\")\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "\n",
    "#RAG().forward(query)\n",
    "#print(lm.inspect_history(n=1))\n",
    "       \n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0389d58-7c3c-4812-b85e-c255996ffb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "result = RAG().forward(\"Is there any mention of optimizing the size of AI/ML models to save storage space and reduce memory usage during inference?\")\n",
    "print(result.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f99faa-1c22-4d2e-9a44-6647fd36cec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Give a concrete 'yes' or 'no' or 'not applicable' answer to a question based on the retrieved context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: Output nothing else apart from either Yes, No, or Not Applicable.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«these properties.\n",
      "2. Capacity Estimation for Dropbox System Design\n",
      "Storage Estimations:\n",
      "Assumptions:Â»\n",
      "[2] Â«The total number of users = 500 million.\n",
      "Total number of daily active users = 100 million\n",
      "The average number of files stored by each user = 200\n",
      "The average size of each file = 100 KB\n",
      "Total number of active connections per minute = 1 million\n",
      "\n",
      "Storage Estimations:\n",
      "\n",
      "Total number of files = 500 million * 200 = 100 billion\n",
      "Total storage required = 100 billion * 100 KB = 10 PB\n",
      "\n",
      "3. High-Level Design(HLD) of Dropbox System Design\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "communicates with the Upload Service on the server side. Large files may be broken into smaller chunks\n",
      "for efficient transfer.Â»\n",
      "[3] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "\n",
      "Question: Is there any mention of optimizing the size of AI/ML models to save storage space and reduce memory usage during inference?\n",
      "\n",
      "Reasoning: Let's think step by step in order to No, based on the provided context which focuses on system design aspects such as capacity estimation, scalability through horizontal scaling, database sharding, cache sharding, file upload handling with large files broken into chunks for efficient transfer. There is no mention of optimizing AI/ML model size or memory usage during inference in this specific text.\n",
      "\n",
      "Answer:\u001b[32m No\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(lm.inspect_history(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52057d1d-dbb7-4796-ab5b-07f04ff6552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                   | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                     | 1/14 [00:39<08:27, 39.03s/it]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                | 2/14 [01:38<10:11, 51.00s/it]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                           | 3/14 [02:13<08:01, 43.78s/it]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                     | 4/14 [02:43<06:22, 38.30s/it]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                | 5/14 [03:16<05:27, 36.43s/it]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                          | 6/14 [03:50<04:45, 35.72s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 7/14 [04:25<04:08, 35.44s/it]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 8/14 [05:04<03:38, 36.44s/it]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 9/14 [05:40<03:01, 36.27s/it]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 10/14 [06:11<02:18, 34.72s/it]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 11/14 [06:35<01:34, 31.38s/it]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 12/14 [07:31<01:17, 38.94s/it]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 13/14 [08:10<00:38, 38.96s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [08:50<00:00, 37.87s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 14 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                    | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Average Metric: 0 / 1  (0.0):   0%|                                                      | 0/7 [00:57<?, ?it/s]\u001b[A\n",
      "Average Metric: 0 / 1  (0.0):  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                       | 1/7 [00:57<05:43, 57.26s/it]\u001b[A\n",
      "Average Metric: 0 / 2  (0.0):  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                       | 1/7 [01:54<05:43, 57.26s/it]\u001b[A\n",
      "Average Metric: 0 / 2  (0.0):  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 2/7 [01:54<04:46, 57.23s/it]\u001b[A\n",
      "Average Metric: 0 / 3  (0.0):  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 2/7 [02:51<04:46, 57.23s/it]\u001b[A\n",
      "Average Metric: 0 / 3  (0.0):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 3/7 [02:51<03:48, 57.24s/it]\u001b[A\n",
      "Average Metric: 0 / 4  (0.0):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 3/7 [03:48<03:48, 57.24s/it]\u001b[A\n",
      "Average Metric: 0 / 4  (0.0):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 4/7 [03:48<02:51, 57.23s/it]\u001b[A\n",
      "Average Metric: 0 / 5  (0.0):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 4/7 [04:46<02:51, 57.23s/it]\u001b[A\n",
      "Average Metric: 0 / 5  (0.0):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 5/7 [04:46<01:54, 57.22s/it]\u001b[A\n",
      "Average Metric: 0 / 6  (0.0):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 5/7 [05:43<01:54, 57.22s/it]\u001b[A\n",
      "Average Metric: 0 / 6  (0.0):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 6/7 [05:43<00:57, 57.22s/it]\u001b[A\n",
      "Average Metric: 0 / 7  (0.0):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 6/7 [06:40<00:57, 57.22s/it]\u001b[A\n",
      "Average Metric: 0 / 7  (0.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [06:40<00:00, 57.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 7  (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/thuva_siva29/green-software-foundation/.venv/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:266: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['False' 'False' 'False' 'False' 'False']' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, metric_name] = df[metric_name].apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9e90f th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_9e90f td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_9e90f_row0_col0, #T_9e90f_row0_col1, #T_9e90f_row0_col2, #T_9e90f_row0_col3, #T_9e90f_row0_col4, #T_9e90f_row1_col0, #T_9e90f_row1_col1, #T_9e90f_row1_col2, #T_9e90f_row1_col3, #T_9e90f_row1_col4, #T_9e90f_row2_col0, #T_9e90f_row2_col1, #T_9e90f_row2_col2, #T_9e90f_row2_col3, #T_9e90f_row2_col4, #T_9e90f_row3_col0, #T_9e90f_row3_col1, #T_9e90f_row3_col2, #T_9e90f_row3_col3, #T_9e90f_row3_col4, #T_9e90f_row4_col0, #T_9e90f_row4_col1, #T_9e90f_row4_col2, #T_9e90f_row4_col3, #T_9e90f_row4_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9e90f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9e90f_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_9e90f_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_9e90f_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_9e90f_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_9e90f_level0_col4\" class=\"col_heading level0 col4\" >answer_exact_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9e90f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9e90f_row0_col0\" class=\"data row0 col0\" >Is there any mention of adopting cloud-native network security tools and controls for efficient security?</td>\n",
       "      <td id=\"T_9e90f_row0_col1\" class=\"data row0 col1\" >Yes</td>\n",
       "      <td id=\"T_9e90f_row0_col2\" class=\"data row0 col2\" >['Each shard is essentially a separate database instance that can be distributed across different servers or\\neven different geographic locations.\\nSharding helps distribute the load, improve query...</td>\n",
       "      <td id=\"T_9e90f_row0_col3\" class=\"data row0 col3\" >To give a concrete example of how to design a scalable cloud storage service that uses sharding with horizontal scaling while ensuring efficient resource usage...</td>\n",
       "      <td id=\"T_9e90f_row0_col4\" class=\"data row0 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9e90f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9e90f_row1_col0\" class=\"data row1 col0\" >Is there any mention of implementing circuit breaker patterns to handle service failures gracefully?</td>\n",
       "      <td id=\"T_9e90f_row1_col1\" class=\"data row1 col1\" >No</td>\n",
       "      <td id=\"T_9e90f_row1_col2\" class=\"data row1 col2\" >['request. We generally mention availability as 5 Nineâ€™s, 4 Nineâ€™s, etc. 5 Nineâ€™s means 99.999% availability,\\n4 Nine means 99.99% availability, and so on.\\nDurability: Durability means...</td>\n",
       "      <td id=\"T_9e90f_row1_col3\" class=\"data row1 col3\" >Implementing Circuit Breaker Patterns in Sharded Database Systems with Concrete Example - A Case Study on a Social Media Platform's User Profile Service To give...</td>\n",
       "      <td id=\"T_9e90f_row1_col4\" class=\"data row1 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9e90f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9e90f_row2_col0\" class=\"data row2 col0\" >Is there any mention of using asynchronous network calls instead of synchronous ones to reduce waiting times?</td>\n",
       "      <td id=\"T_9e90f_row2_col1\" class=\"data row2 col1\" >Yes</td>\n",
       "      <td id=\"T_9e90f_row2_col2\" class=\"data row2 col2\" >['-----\\n\\n4.3. Message Queuing Service\\nThe messaging service queue will be responsible for the asynchronous communication between the\\nclients and the synchronization service.\\n\\ndesign-dropbox-4\\n\\nBelow are the main requirements of...</td>\n",
       "      <td id=\"T_9e90f_row2_col3\" class=\"data row2 col3\" >In a distributed system designed for an eCommerce platform handling millions of transactions daily with thousands of concurrent users and high-volume message processing requirements, implementing...</td>\n",
       "      <td id=\"T_9e90f_row2_col4\" class=\"data row2 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9e90f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_9e90f_row3_col0\" class=\"data row3 col0\" >Is there any mention of scheduling Kubernetes cron jobs during off-peak hours to optimize resource usage?</td>\n",
       "      <td id=\"T_9e90f_row3_col1\" class=\"data row3 col1\" >No</td>\n",
       "      <td id=\"T_9e90f_row3_col2\" class=\"data row3 col2\" >['7. Scalabilty for Dropbox System Design\\nHorizontal Scaling\\nWe can add more servers behind the load balancer to increase the capacity of each service. This is\\nknown as...</td>\n",
       "      <td id=\"T_9e90f_row3_col3\" class=\"data row3 col3\" >I apologize if my previous response did not directly address your question. It seems there might have been a misunderstanding in the task description as...</td>\n",
       "      <td id=\"T_9e90f_row3_col4\" class=\"data row3 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9e90f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_9e90f_row4_col0\" class=\"data row4 col0\" >Is there any mention of using DDoS protection to maintain service availability?</td>\n",
       "      <td id=\"T_9e90f_row4_col1\" class=\"data row4 col1\" >No</td>\n",
       "      <td id=\"T_9e90f_row4_col2\" class=\"data row4 col2\" >['request. We generally mention availability as 5 Nineâ€™s, 4 Nineâ€™s, etc. 5 Nineâ€™s means 99.999% availability,\\n4 Nine means 99.99% availability, and so on.\\nDurability: Durability means...</td>\n",
       "      <td id=\"T_9e90f_row4_col3\" class=\"data row4 col3\" >Certainly! Here's an example scenario where we can implement horizontal scaling for handling high volumes of messages while maintaining ACID properties using Redis, which is...</td>\n",
       "      <td id=\"T_9e90f_row4_col4\" class=\"data row4 col4\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff5f8619d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 2 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
    "\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=trainset)\n",
    "\n",
    "evaluate_on_dataset = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "#Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_on_dataset(compiled_rag, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2058bd8b-70e2-4d47-8608-0d146ef8b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Give a concrete 'yes' or 'no' or 'not applicable' answer to a question based on the retrieved context.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Is there any mention of scaling down Kubernetes applications when not in use to reduce resource usage?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of managing peak CPU utilization to avoid over-provisioning?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of adopting serverless cloud services to optimize resource usage?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of setting storage retention policies to automatically clean up old data?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of scaling infrastructure based on real-time user demand?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of optimizing average CPU utilization to ensure efficient use of resources?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of removing unused assets to optimize performance?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of queuing non-urgent processing requests to reduce peak loads?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of allowing independent scaling of logical components?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of reducing transmitted data to save energy?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of optimizing storage utilization to reduce energy consumption?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of minimizing the total number of deployed environments to save resources?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: Output nothing else apart from either Yes, No, or Not Applicable.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "[2] Â«Scaling can be achieved using techniques such as vertical scaling (increasing hardware capabilities) or\n",
      "horizontal scaling (adding more machines).\n",
      "However, horizontal scaling for relational databases often involves complexities, especially in scenarios\n",
      "with high read and write operations.\n",
      "4.2.2 Database Sharding:\n",
      "Database sharding is a horizontal partitioning technique where a large database is divided into smaller,\n",
      "more manageable parts called shards.Â»\n",
      "[3] Â«-----\n",
      "\n",
      "The metadata database maintains the indexes of the various chunks. The information contains\n",
      "files/chunks names, and their different versions along with the information of users and workspace.\n",
      "\n",
      "You can use RDBMS or NoSQL but make sure that you meet the data consistency property because\n",
      "multiple clients will be working on the same file.\n",
      "With RDBMS there is no problem with the consistency but with NoSQL, you will get eventual\n",
      "consistency.\n",
      "Lets understand how we can efficientlt do relational database scaling\n",
      "\n",
      "4.2.1 Relational Database Scaling:\n",
      "Relational databases like MySQL may face scalability challenges as the data and traffic grow.Â»\n",
      "\n",
      "Question: Is there any mention of scaling down applications during idle periods to minimize resource usage?\n",
      "\n",
      "Reasoning: Let's think step by step in order to No, based on the provided context. There is no explicit or implicit suggestion about scaling down Kubernetes applications when not in use for reducing resource usage mentioned within it. The text discusses horizontal and vertical scaling techniques as well as database sharding but does not mention auto-scaling of services according to demand which would include scaling down during idle periods.\n",
      "\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«request. We generally mention availability as 5 Nineâ€™s, 4 Nineâ€™s, etc. 5 Nineâ€™s means 99.999% availability,\n",
      "4 Nine means 99.99% availability, and so on.\n",
      "Durability: Durability means the data uploaded by the user should be permanently stored in the database\n",
      "even in case of database failure. Our System should ensure the files uploaded by the user should be\n",
      "permanently stored on the drive without any data loss.\n",
      "Reliability: Reliability means how many times the system gives the expected output for the same input.\n",
      "Scalability: With the growing number of users our system should be capable enough to handle the\n",
      "increasing traffic.\n",
      "ACID properties: Atomicity, Consistency, Integrity and Durability. All the file operations should follow\n",
      "these properties.Â»\n",
      "[2] Â«-----\n",
      "\n",
      "4.3. Message Queuing Service\n",
      "The messaging service queue will be responsible for the asynchronous communication between the\n",
      "clients and the synchronization service.\n",
      "\n",
      "design-dropbox-4\n",
      "\n",
      "Below are the main requirements of the Message Queuing Service.\n",
      "\n",
      "Ability to handle lots of reading and writing requests.\n",
      "Store lots of messages in a highly available and reliable queue.\n",
      "High performance and high scalability.\n",
      "Provides load balancing and elasticity for multiple instances of the Synchronization Service.\n",
      "There will be two types of messaging queues in the service.Â»\n",
      "[3] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "\n",
      "Question: Is there any mention of using DDoS protection to maintain service availability?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m As an AI developed before 2021, I cannot provide real-time or future predictions. However, here is a concrete example that fits the criteria you provided based on your request for information about DDoS protection and scalability within a messaging service system:\n",
      "\n",
      "In our hypothetical message queueing application designed to handle high volumes of messages with 99.99% availability requirements using Redis, we can implement several strategies that leverage the mentioned features while ensuring ACID properties (Atomicity, Consistency, Isolation and Durability) are maintained:\n",
      "\n",
      "1. **Horizontal Scaling** - To handle lots of read/write requests efficiently in a scalable manner\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Give a concrete 'yes' or 'no' or 'not applicable' answer to a question based on the retrieved context.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Is there any mention of scaling down Kubernetes applications when not in use to reduce resource usage?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of managing peak CPU utilization to avoid over-provisioning?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of adopting serverless cloud services to optimize resource usage?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of setting storage retention policies to automatically clean up old data?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of scaling infrastructure based on real-time user demand?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of optimizing average CPU utilization to ensure efficient use of resources?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of removing unused assets to optimize performance?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of queuing non-urgent processing requests to reduce peak loads?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of allowing independent scaling of logical components?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of reducing transmitted data to save energy?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of optimizing storage utilization to reduce energy consumption?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of minimizing the total number of deployed environments to save resources?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: Output nothing else apart from either Yes, No, or Not Applicable.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "[2] Â«Scaling can be achieved using techniques such as vertical scaling (increasing hardware capabilities) or\n",
      "horizontal scaling (adding more machines).\n",
      "However, horizontal scaling for relational databases often involves complexities, especially in scenarios\n",
      "with high read and write operations.\n",
      "4.2.2 Database Sharding:\n",
      "Database sharding is a horizontal partitioning technique where a large database is divided into smaller,\n",
      "more manageable parts called shards.Â»\n",
      "[3] Â«-----\n",
      "\n",
      "The metadata database maintains the indexes of the various chunks. The information contains\n",
      "files/chunks names, and their different versions along with the information of users and workspace.\n",
      "\n",
      "You can use RDBMS or NoSQL but make sure that you meet the data consistency property because\n",
      "multiple clients will be working on the same file.\n",
      "With RDBMS there is no problem with the consistency but with NoSQL, you will get eventual\n",
      "consistency.\n",
      "Lets understand how we can efficientlt do relational database scaling\n",
      "\n",
      "4.2.1 Relational Database Scaling:\n",
      "Relational databases like MySQL may face scalability challenges as the data and traffic grow.Â»\n",
      "\n",
      "Question: Is there any mention of scaling down applications during idle periods to minimize resource usage?\n",
      "\n",
      "Reasoning: Let's think step by step in order to No, based on the provided context. There is no explicit or implicit suggestion about scaling down Kubernetes applications when not in use for reducing resource usage mentioned within it. The text discusses horizontal and vertical scaling techniques as well as database sharding but does not mention auto-scaling of services according to demand which would include scaling down during idle periods.\n",
      "\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«request. We generally mention availability as 5 Nineâ€™s, 4 Nineâ€™s, etc. 5 Nineâ€™s means 99.999% availability,\n",
      "4 Nine means 99.99% availability, and so on.\n",
      "Durability: Durability means the data uploaded by the user should be permanently stored in the database\n",
      "even in case of database failure. Our System should ensure the files uploaded by the user should be\n",
      "permanently stored on the drive without any data loss.\n",
      "Reliability: Reliability means how many times the system gives the expected output for the same input.\n",
      "Scalability: With the growing number of users our system should be capable enough to handle the\n",
      "increasing traffic.\n",
      "ACID properties: Atomicity, Consistency, Integrity and Durability. All the file operations should follow\n",
      "these properties.Â»\n",
      "[2] Â«-----\n",
      "\n",
      "4.3. Message Queuing Service\n",
      "The messaging service queue will be responsible for the asynchronous communication between the\n",
      "clients and the synchronization service.\n",
      "\n",
      "design-dropbox-4\n",
      "\n",
      "Below are the main requirements of the Message Queuing Service.\n",
      "\n",
      "Ability to handle lots of reading and writing requests.\n",
      "Store lots of messages in a highly available and reliable queue.\n",
      "High performance and high scalability.\n",
      "Provides load balancing and elasticity for multiple instances of the Synchronization Service.\n",
      "There will be two types of messaging queues in the service.Â»\n",
      "[3] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "\n",
      "Question: Is there any mention of using DDoS protection to maintain service availability?\n",
      "\n",
      "Reasoning: Let's think step by step in order to As an AI developed before 2021, I cannot provide real-time or future predictions. However, here is a concrete example that fits the criteria you provided based on your request for information about DDoS protection and scalability within a messaging service system: In our hypothetical message queueing application designed to handle high volumes of messages with 99.99% availability requirements using Redis, we can implement several strategies that leverage the mentioned features while ensuring ACID properties (Atomicity, Consistency, Isolation and Durability) are maintained: 1. **Horizontal Scaling** - To handle lots of read/write requests efficiently in a scalable manner\n",
      "\n",
      "Answer:\u001b[32m Certainly! Here's an example scenario where we can implement horizontal scaling for handling high volumes of messages while maintaining ACID properties using Redis, which is often used as a message queue system due to its performance and speed. Let me outline the steps involved in designing such a service with these considerations:\n",
      "\n",
      "1. **Horizontal Scalability** - To ensure that our messaging service can handle lots of read/write requests efficiently while maintaining high availability, we'll use Redis Cluster to distribute messages across multiple servers (servers). This will allow us to horizontally scale the system by adding more nodes as needed.\n",
      "\n",
      "2. **Redis Sharding** - We can sh\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Give a concrete 'yes' or 'no' or 'not applicable' answer to a question based on the retrieved context.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Is there any mention of scaling down Kubernetes applications when not in use to reduce resource usage?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of managing peak CPU utilization to avoid over-provisioning?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of adopting serverless cloud services to optimize resource usage?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of setting storage retention policies to automatically clean up old data?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of scaling infrastructure based on real-time user demand?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of optimizing average CPU utilization to ensure efficient use of resources?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of removing unused assets to optimize performance?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of queuing non-urgent processing requests to reduce peak loads?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of allowing independent scaling of logical components?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of reducing transmitted data to save energy?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of optimizing storage utilization to reduce energy consumption?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of minimizing the total number of deployed environments to save resources?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: Output nothing else apart from either Yes, No, or Not Applicable.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "[2] Â«Scaling can be achieved using techniques such as vertical scaling (increasing hardware capabilities) or\n",
      "horizontal scaling (adding more machines).\n",
      "However, horizontal scaling for relational databases often involves complexities, especially in scenarios\n",
      "with high read and write operations.\n",
      "4.2.2 Database Sharding:\n",
      "Database sharding is a horizontal partitioning technique where a large database is divided into smaller,\n",
      "more manageable parts called shards.Â»\n",
      "[3] Â«-----\n",
      "\n",
      "The metadata database maintains the indexes of the various chunks. The information contains\n",
      "files/chunks names, and their different versions along with the information of users and workspace.\n",
      "\n",
      "You can use RDBMS or NoSQL but make sure that you meet the data consistency property because\n",
      "multiple clients will be working on the same file.\n",
      "With RDBMS there is no problem with the consistency but with NoSQL, you will get eventual\n",
      "consistency.\n",
      "Lets understand how we can efficientlt do relational database scaling\n",
      "\n",
      "4.2.1 Relational Database Scaling:\n",
      "Relational databases like MySQL may face scalability challenges as the data and traffic grow.Â»\n",
      "\n",
      "Question: Is there any mention of scaling down applications during idle periods to minimize resource usage?\n",
      "\n",
      "Reasoning: Let's think step by step in order to No, based on the provided context. There is no explicit or implicit suggestion about scaling down Kubernetes applications when not in use for reducing resource usage mentioned within it. The text discusses horizontal and vertical scaling techniques as well as database sharding but does not mention auto-scaling of services according to demand which would include scaling down during idle periods.\n",
      "\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "[2] Â«creating, updating, or deleting files/folders.\n",
      "It gives a notification to the indexer and chunker if any action is performed in the files or folders.\n",
      "Chunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage\n",
      "with a unique id or hash of these chunks.\n",
      "To recreate the files these chunks can be joined together. For any changes in the files, the chunking\n",
      "algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud\n",
      "storage.\n",
      "It reduces bandwidth usage, synchronization time, and storage space in the cloud.\n",
      "Indexer is responsible for updating the internal database when it receives the notification from the watcher\n",
      "(for any action performed in folders/files).Â»\n",
      "[3] Â«Each shard is essentially a separate database instance that can be distributed across different servers or\n",
      "even different geographic locations.\n",
      "Sharding helps distribute the load, improve query performance, and enhance scalability.\n",
      "4.2.3 Challenges with Database Sharding:\n",
      "Managing multiple shards can become complex, especially when updates or new information needs to be\n",
      "added. Coordinating transactions across shards can be challenging. Maintenance, backup, and recovery\n",
      "operations become more intricate.\n",
      "\n",
      "4.2.4 Edge Wrapper:\n",
      "An edge wrapper is an abstraction layer that sits between the application and the sharded databases.Â»\n",
      "\n",
      "Question: Is there any mention of scheduling Kubernetes cron jobs during off-peak hours to optimize resource usage?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m No, I cannot provide a concrete solution for this task as it seems unrelated to the provided document. The given text does not contain any specific details or context about scheduling Kubernetes cron jobs during off-peak hours within its content related to database sharding and Redis cache management strategies in distributed systems like Apache Ignite, which is a key-value store for managing metadata of files/folders on the cloud. The text seems more focused on discussing how chunked file handling can be used with Kubernetes clusters as part of an application's architecture to manage large datasets efficiently and securely without mentioning anything about scheduling or time constraints like off-peak hours for cron jobs, which are typically associated with databases\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Give a concrete 'yes' or 'no' or 'not applicable' answer to a question based on the retrieved context.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Is there any mention of scaling down Kubernetes applications when not in use to reduce resource usage?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of managing peak CPU utilization to avoid over-provisioning?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of adopting serverless cloud services to optimize resource usage?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of setting storage retention policies to automatically clean up old data?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?\n",
      "Answer: Not Applicable\n",
      "\n",
      "Question: Is there any mention of scaling infrastructure based on real-time user demand?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of optimizing average CPU utilization to ensure efficient use of resources?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of removing unused assets to optimize performance?\n",
      "Answer: No\n",
      "\n",
      "Question: Is there any mention of queuing non-urgent processing requests to reduce peak loads?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of allowing independent scaling of logical components?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of reducing transmitted data to save energy?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of optimizing storage utilization to reduce energy consumption?\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is there any mention of minimizing the total number of deployed environments to save resources?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: Output nothing else apart from either Yes, No, or Not Applicable.\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "[2] Â«Scaling can be achieved using techniques such as vertical scaling (increasing hardware capabilities) or\n",
      "horizontal scaling (adding more machines).\n",
      "However, horizontal scaling for relational databases often involves complexities, especially in scenarios\n",
      "with high read and write operations.\n",
      "4.2.2 Database Sharding:\n",
      "Database sharding is a horizontal partitioning technique where a large database is divided into smaller,\n",
      "more manageable parts called shards.Â»\n",
      "[3] Â«-----\n",
      "\n",
      "The metadata database maintains the indexes of the various chunks. The information contains\n",
      "files/chunks names, and their different versions along with the information of users and workspace.\n",
      "\n",
      "You can use RDBMS or NoSQL but make sure that you meet the data consistency property because\n",
      "multiple clients will be working on the same file.\n",
      "With RDBMS there is no problem with the consistency but with NoSQL, you will get eventual\n",
      "consistency.\n",
      "Lets understand how we can efficientlt do relational database scaling\n",
      "\n",
      "4.2.1 Relational Database Scaling:\n",
      "Relational databases like MySQL may face scalability challenges as the data and traffic grow.Â»\n",
      "\n",
      "Question: Is there any mention of scaling down applications during idle periods to minimize resource usage?\n",
      "\n",
      "Reasoning: Let's think step by step in order to No, based on the provided context. There is no explicit or implicit suggestion about scaling down Kubernetes applications when not in use for reducing resource usage mentioned within it. The text discusses horizontal and vertical scaling techniques as well as database sharding but does not mention auto-scaling of services according to demand which would include scaling down during idle periods.\n",
      "\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] Â«7. Scalabilty for Dropbox System Design\n",
      "Horizontal Scaling\n",
      "We can add more servers behind the load balancer to increase the capacity of each service. This is\n",
      "known as Horizontal Scaling and each service can be independently scaled horizontally in our design.\n",
      "Database Sharding\n",
      "Metadata DB is sharded based on object_id. Our hash function will map each object_id to a random\n",
      "server where we can store the file/folder metadata. To query for a particular object_id, service can\n",
      "determine the database server using same hash function and query for data. This approach will distribute\n",
      "our database load to multiple servers making it scalable.\n",
      "Cache Sharding\n",
      "Similar to Metadata DB Sharding, we are distributing the cache to multiple servers. In-fact Redis has outÂ»\n",
      "[2] Â«creating, updating, or deleting files/folders.\n",
      "It gives a notification to the indexer and chunker if any action is performed in the files or folders.\n",
      "Chunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage\n",
      "with a unique id or hash of these chunks.\n",
      "To recreate the files these chunks can be joined together. For any changes in the files, the chunking\n",
      "algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud\n",
      "storage.\n",
      "It reduces bandwidth usage, synchronization time, and storage space in the cloud.\n",
      "Indexer is responsible for updating the internal database when it receives the notification from the watcher\n",
      "(for any action performed in folders/files).Â»\n",
      "[3] Â«Each shard is essentially a separate database instance that can be distributed across different servers or\n",
      "even different geographic locations.\n",
      "Sharding helps distribute the load, improve query performance, and enhance scalability.\n",
      "4.2.3 Challenges with Database Sharding:\n",
      "Managing multiple shards can become complex, especially when updates or new information needs to be\n",
      "added. Coordinating transactions across shards can be challenging. Maintenance, backup, and recovery\n",
      "operations become more intricate.\n",
      "\n",
      "4.2.4 Edge Wrapper:\n",
      "An edge wrapper is an abstraction layer that sits between the application and the sharded databases.Â»\n",
      "\n",
      "Question: Is there any mention of scheduling Kubernetes cron jobs during off-peak hours to optimize resource usage?\n",
      "\n",
      "Reasoning: Let's think step by step in order to No, I cannot provide a concrete solution for this task as it seems unrelated to the provided document. The given text does not contain any specific details or context about scheduling Kubernetes cron jobs during off-peak hours within its content related to database sharding and Redis cache management strategies in distributed systems like Apache Ignite, which is a key-value store for managing metadata of files/folders on the cloud. The text seems more focused on discussing how chunked file handling can be used with Kubernetes clusters as part of an application's architecture to manage large datasets efficiently and securely without mentioning anything about scheduling or time constraints like off-peak hours for cron jobs, which are typically associated with databases\n",
      "\n",
      "Answer:\u001b[32m I apologize if my previous response did not directly address your question. It seems there might have been a misunderstanding in the task description as it appears you're asking to create an instruction based on concrete examples related to Kubernetes and Redis cache management strategies, which is outside of what was provided in the document excerpt about database sharding with Apache Ignite.\n",
      "\n",
      "However, I understand that your request seems more aligned towards a different topic involving scheduling tasks within cloud storage systems or databases rather than specifically discussing how to schedule Kubernetes cron jobs for managing Redis cache data during off-peak hours. If you're looking for information on this subject, here is an example of what such instructions could look like\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(lm.inspect_history(n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aaffb6b-406d-40b8-aa96-ac5284583998",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag.save(\"file.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
