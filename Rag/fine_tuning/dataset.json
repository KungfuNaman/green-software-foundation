[{
    "query": "Is there any mention of allowing independent scaling of logical components?",
    "context": "This combination of edge wrapper and ORM simplifies database management, making it easier to scale the application horizontally with sharded databases while maintaining a cohesive and developer-friendly interface.",
    "explanation": "The context mentions the combination of edge wrapper and ORM simplifying database management, which facilitates horizontal scaling with sharded databases. This indicates that independent scaling of logical components is supported as the system can add capacity by adding more servers without disrupting the existing setup.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of queuing non-urgent processing requests?",
    "context": "The messaging service queue will be responsible for the asynchronous communication between the clients and the synchronization service.",
    "explanation": "The context mentions that the messaging service queue handles asynchronous communication between clients and the synchronization service, which implies that non-urgent processing requests are queued to ensure efficient handling.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of using asynchronous network calls instead of synchronous ones?",
    "context": "The messaging service queue will be responsible for the asynchronous communication between the clients and the synchronization service.",
    "explanation": "From the context, it can be derived that asynchronous network calls are used for communication between clients and the synchronization service, indicating the use of asynchronous calls.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of adopting cloud-native network security tools and controls?",
    "context": "The client application requests a Presigned URL from the Upload Service. The server generates the Presigned URL by interacting with the S3 service, creating a unique token for the specific upload operation. These URLs grant temporary, secure access to upload a specific file to a designated S3 bucket.",
    "explanation": "The context mentions the use of Presigned URLs for secure file uploads to an S3 bucket. This approach involves generating unique, time-limited tokens that provide secure access to cloud storage resources, indicating the adoption of cloud-native security controls to safeguard data during upload operations.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of implementing a stateless design?",
    "context": "Request Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\ndesign-dropbox-2\n\nLet\u2019s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it is received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  new_objects: [\n    {\n      object_id:\n      object_type:\n      name:\n      chunk_ids: [\n        chunk1,\n        chunk2,\n        chunk3\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing the name of the object, object id, object type, and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.",
    "explanation": "The context does not discuss implementing a stateless design for scalability.",
    "judgement": "No"
},
{
    "query": "Is there any mention of scaling down Kubernetes applications when not in use?",
    "context": "Request Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\ndesign-dropbox-2\n\nLetâ€™s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nYou just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.",
    "explanation": "The context does not mention the use of Kubernetes in the system, and therefore, considering whether Kubernetes applications are scaled down when not in use is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling infrastructure based on real-time user demand?",
    "context": "Request Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\ndesign-dropbox-2\n\nLet\u2019s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it is received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  new_objects: [\n    {\n      object_id:\n      object_type:\n      name:\n      chunk_ids: [\n        chunk1,\n        chunk2,\n        chunk3\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing the name of the object, object id, object type, and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.",
    "explanation": "There is no mention in the context of scaling infrastructure based on real-time user demand.",
    "judgement": "No"
},
{
    "query": "Is there any mention of optimizing the size of AI/ML models?",
    "context": "We can break the files into multiple chunks to overcome the problem we discussed above. There is no need to upload/download the whole single file after making any changes in the file.\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it will be received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.\n\n---\n\nYou just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.\n\n---\n\nRequest Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).",
    "explanation": "Artifical Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of leveraging pre-trained models and transfer learning?",
    "context": "design-dropbox-2\n\nLetâ€™s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\nRequest Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nYou just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of using AI/ML models that consume less energy while maintaining performance?",
    "context": "You just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.\n\n---\n\nRequest Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\nNon Functional Requirements:\nAvailability: Availability means what percentage of the time the system is available to take a userâ€™s request. We generally mention availability as 5 Nineâ€™s, 4 Nineâ€™s, etc. 5 Nineâ€™s means 99.999% availability, 4 Nineâ€™s means 99.99% availability, and so on.\nDurability: Durability means the data uploaded by the user should be permanently stored in the database even in case of database failure. Our System should ensure the files uploaded by the user should be permanently stored on the drive without any data loss.\nReliability: Reliability means how many times the system gives the expected output for the same input.\nScalability: With the growing number of users our system should be capable enough to handle the increasing traffic.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it will be received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling down Kubernetes applications when not in use?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:",
    "explanation": "The context does not mention the use of Kubernetes in the system, and therefore, considering whether Kubernetes applications are scaled down when not in use is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling infrastructure based on real-time user demand?",
    "context": "We will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.",
    "explanation": "The context mentions the use of a microservices architecture to facilitate horizontal scaling and decoupling of services, indicating the ability to scale infrastructure based on real-time user demand by adding or removing instances of services as needed.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?",
    "context": "Download Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nThe responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.",
    "explanation": "The context does not mention the use of Kubernetes in the system and therefore, considering whether relevant metrics are used to dynamically adjust Kubernetes workloads is irrelevant",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scheduling Kubernetes cron jobs during off-peak hours?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.",
    "explanation": "The context does not mention the use of Kubernetes in the system and therefore, considering whether Kubernetes cron jobs are scheduled during off-peak hours is irrelevant.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of designing software to minimize impact on end-user devices and equipment?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nImportant Topics for WhatsApp System Design\n\nRequirements\nCapacity Estimation\nHigh Level Design (HLD) of WhatsApp Messenger\nData Model Design:\nAPI Design\nLow Level Design (LLD) of System Design\nApproach to achieve the below system attributes\nRequirements\nThe WhatsApp messenger design should meet below requirements:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month",
    "explanation": "The context does not mention designing software to minimize impact on end-user devices and equipment.",
    "judgement": "No"
},
{
    "query": "Is there any mention of choosing server regions that are closest to users?",
    "context": "The group service keeps all information about users in each group in the system. It has all the information about each group, including user IDs, group ID, status, group icon, number of users, and so on. This service resides on top of the MySQL database cluster, with multiple secondary replicas distributed geographically. A Redis cache server also exists to cache data from the MySQL servers. Both geographically distributed replicas and Redis cache aid in reducing latency.",
    "explanation": "The context mentions that the service uses geographically distributed replicas of the MySQL database cluster and Redis cache servers to reduce latency. This implies choosing server regions closer to users.",
    "judgement": "Yes"
},
{
    "query":"Is there any mention of deleting unused storage resources?",
    "context":"Message service is a repository of messages on top of the Mnesia database cluster. It acts as an interface to the Mnesia database for other services interacting with the databases. It is responsible for storing and retrieving messages from the Mnesia database. It also deletes messages from the Mnesia database after a configurable amount of time.",
    "explanation":"The context mentions that messages are deleted from the Mnesia database after configurable amount of time",
    "judgement":"Yes"
},
{
    "query":"Is there any mention of caching static data?",
    "context":"A Redis cache server also exists to cache data from the MySQL servers. Both geographically distributed replicas and Redis cache aid in reducing latency.",
    "explanation":"The context mentions the use of Redis, a cache server, which indicates that static data is being cached in the system.",
    "judgement":"Yes"
},
{
    "query":"Is there any mention of compressing stored data?",
    "context":"The media file is compressed and encrypted on the device side. The compressed and encrypted file is sent to the asset service to store the file on blob storage.",
    "explanation":"The context mentions that media files are compressed on the device side before being sent to the asset service for storage. This reduces the amount of data stored, thus improving data efficiency.",
    "judgement":"Yes"
},
{
    "query":"Is there any mention of compressing data transmitted over networks?",
    "context":"The media file is compressed and encrypted on the device side. The compressed and encrypted file is sent to the asset service to store the file on blob storage.",
    "explanation":"The context mentions that media files are compressed on the device side before being transmitted to the asset service, which indicates an effort to minimize the amount of data transmitted over the network.",
    "judgement":"Yes"
},
{
    "query": "Is there any mention of implementing a stateless design?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30âˆ—10 TB/day = 300 TB/month",
    "explanation": "The context does not mention implementing a stateless design for scalability. Implementing a stateless design is relevant for such an application.",
    "judgement": "No"
},
{
    "query": "Is there any mention of matching service level objectives to business needs?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nImportant Topics for WhatsApp System Design\n\nRequirements\nCapacity Estimation\nHigh Level Design (HLD) of WhatsApp Messenger\nData Model Design:\nAPI Design\nLow Level Design (LLD) of System Design\nApproach to achieve the below system attributes\nRequirements\nThe WhatsApp messenger design should meet below requirements:",
    "explanation": "The context does not mention matching service level objectives to business needs. The query is still relevant to the system but is not addressed in the provided context.",
    "judgement": "No"
},
{
    "query": "Is there any mention of deploying AI models on edge devices?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30âˆ—10 TB/day = 300 TB/month",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of adopting a serverless architecture for AI/ML workload processes?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30âˆ—10 TB/day = 300 TB/month\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nImportant Topics for WhatsApp System Design\n\nRequirements\nCapacity Estimation\nHigh Level Design (HLD) of WhatsApp Messenger\nData Model Design:\nAPI Design\nLow Level Design (LLD) of System Design\nApproach to achieve the below system attributes\nRequirements\nThe WhatsApp messenger design should meet below requirements:",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling infrastructure based on real-time user demand?",
    "context": "Treat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nThe data is shared across the cluster within the same zone and multiple copies of the cache are stored in sharded nodes.\nEvery time when write happens to the client all the nodes in all the clusters are updated but when the read happens to the cache, it is only sent to the nearest cluster (not all the cluster and nodes) and its nodes.\nIn case, a node is not available then read from a different available node. This approach increases performance, availability, and reliability.\n3.4. Data Processing in Netflix Using Kafka And Apache Chukwa\nWhen you click on a video Netflix starts processing data in various terms and it takes less than a nanosecond. Letâ€™s discuss how the evolution pipeline works on Netflix.\n\n---\n\nAn outbound filter is used for zipping the content, calculating the metrics, or adding/removing custom headers. After that, the response is sent back to the Netty server and then it is received by the client.\nAdvantages of using ZUUL:\n\n---\n\nmysql\n\nAll the read queries are redirected to the read replicas and only the write queries are redirected to the master nodes.\n\nIn the case of a primary master MySQL failure, the secondary master node will take over the primary role, and the route53 (DNS configuration) entry for the database will be changed to this new primary node.\nThis will also redirect the write queries to this new primary master node.\n4.2. Cassandra\nCassandra is a NoSQL database that can handle large amounts of data and it can also handle heavy writing and reading. When Netflix started acquiring more users, the viewing history data for each member also started increasing. This increases the total number of viewing history data and it becomes challenging for Netflix to handle this massive amount of data.",
    "explanation": "The context does not specifically mention scaling infrastructure based on real-time user demand. While it discusses general scaling and fault tolerance strategies, it does not explicitly cover real-time scaling based on user demand.",
    "judgement": "No"
},
{
    "query": "Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?",
    "context": "Netflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nev-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.\n\n---\n\nIt includes a lot of powerful and flexible toolkits to display, monitor, and analyze the result.\nChukwe collects the events from different parts of the system and from Chukwe you can do monitoring and analysis or you can use the dashboard to view the events.\nChukwe writes the event in the Hadoop file sequence format (S3). After that Big Data team processes these S3 Hadoop files and writes Hive in Parquet data format.\nThis process is called batch processing which basically scans the whole data at the hourly or daily frequency.\nTo upload online events to EMR/S3, Chukwa also provide traffic to Kafka (the main gate in real-time data processing).\n\n---\n\nWhen you load the front page you see multiple rows of different kinds of movies. Netflix personalizes this data and decides what kind of rows or what kind of movies should be displayed to a specific user. This data is based on the userâ€™s historical data and preferences.\n\nAlso, for that specific user, Netflix performs sorting of the movies and calculates the relevance ranking (for the recommendation) of these movies available on their platform. In Netflix, Apache Spark is used for content recommendations and personalization.\n\nA majority of the machine learning pipelines are run on these large spark clusters. These pipelines are then used to do row selection, sorting, title relevance ranking, and artwork personalization among others.\n\n---\n\nTreat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?",
    "explanation": "The context does not mention the use of Kubernetes in the system and therefore, considering whether relevant metrics are used to dynamically adjust Kubernetes workloads is irrelevant.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of reducing transmitted data?",
    "context": "This happens because the application keeps checking the best streaming open connect server and switches between formats (for the best viewing experience) when itâ€™s needed. \nUser data is saved in AWS such as searches, viewing, location, device, reviews, and likes, Netflix uses it to build the movie recommendation for users using the Machine learning model or Hadoop.\n\n---\n\n3.2. How Netflix balance the high traffic load\n1. Elastic Load Balancer\nelastic-load-balancer\n\nELB in Netflix is responsible for routing the traffic to front-end services. ELB performs a two-tier load-balancing scheme where the load is balanced over zones first and then instances (servers).\n\n---\n\n2.1. Microservices Architecture of Netflix \nNetflixâ€™s architectural style is built as a collection of services. This is known as microservices architecture and this power all of the APIs needed for applications and Web apps. When the request arrives at the endpoint it calls the other microservices for required data and these microservices can also request the data from different microservices. After that, a complete response for the API request is sent back to the endpoint.\n\n---\n\n3.1. How Does Netflix Onboard a Movie/Video?\nNetflix receives very high-quality videos and content from the production houses, so before serving the videos to the users it does some preprocessing.\n\n---\n\nNetflix scaled the storage of viewing history data-keeping two main goals in their mind:\n\nSmaller Storage Footprint.\nConsistent Read/Write Performance as viewing per member grows (viewing history data write-to-read ratio is about 9:1 in Cassandra).\ncasandra-service-pattern\n\nTotal Denormalized Data Model  \nOver 50 Cassandra Clusters\nOver 500 Nodes\nOver 30TB of daily backups\nThe biggest cluster has 72 nodes.\n1 cluster over 250K writes/s\nInitially, the viewing history was stored in Cassandra in a single row. When the number of users started increasing on Netflix the row sizes as well as the overall data size increased. This resulted in high storage, more operational cost, and slow performance of the application. The solution to this problem was to compress the old rows.",
    "explanation": "The context does not mention any specific methods for reducing transmitted data. While there is information on data storage and processing, it does not address strategies for minimizing the amount of data transmitted over the network.",
    "judgement": "No"
},
{
    "query": "Is there any mention of scheduling Kubernetes cron jobs during off-peak hours?",
    "context": "Treat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nAn outbound filter is used for zipping the content, calculating the metrics, or adding/removing custom headers. After that, the response is sent back to the Netty server and then it is received by the client.\nAdvantages of using ZUUL:\n\n---\n\nThe data is shared across the cluster within the same zone and multiple copies of the cache are stored in sharded nodes.\nEvery time when write happens to the client all the nodes in all the clusters are updated but when the read happens to the cache, it is only sent to the nearest cluster (not all the cluster and nodes) and its nodes.\nIn case, a node is not available then read from a different available node. This approach increases performance, availability, and reliability.\n3.4. Data Processing in Netflix Using Kafka And Apache Chukwa\nWhen you click on a video Netflix starts processing data in various terms and it takes less than a nanosecond. Letâ€™s discuss how the evolution pipeline works on Netflix.\n\n---\n\nev-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.",
    "explanation": "The context does not mention the use of Kubernetes in the system, making this query irrelevant.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of using asynchronous network calls instead of synchronous ones?",
    "context": "When you load the front page you see multiple rows of different kinds of movies. Netflix personalizes this data and decides what kind of rows or what kind of movies should be displayed to a specific user. This data is based on the userâ€™s historical data and preferences.\n\nAlso, for that specific user, Netflix performs sorting of the movies and calculates the relevance ranking (for the recommendation) of these movies available on their platform. In Netflix, Apache Spark is used for content recommendations and personalization.\n\nA majority of the machine learning pipelines are run on these large spark clusters. These pipelines are then used to do row selection, sorting, title relevance ranking, and artwork personalization among others.\n\n---\n\nev-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.\n\n---\n\nmysql\n\nAll the read queries are redirected to the read replicas and only the write queries are redirected to the master nodes.\n\nIn the case of a primary master MySQL failure, the secondary master node will take over the primary role, and the route53 (DNS configuration) entry for the database will be changed to this new primary node.\nThis will also redirect the write queries to this new primary master node.  \n4.2. Cassandra\nCassandra is a NoSQL database that can handle large amounts of data and it can also handle heavy writing and reading. When Netflix started acquiring more users, the viewing history data for each member also started increasing. This increases the total number of viewing history data and it becomes challenging for Netflix to handle this massive amount of data.\n\n---\n\nTreat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.",
    "explanation": "The context does not mention the use of asynchronous network calls instead of synchronous ones.",
    "judgement": "No"
},
{
    "query": "Is there any mention of implementing circuit breaker patterns?",
    "context": "Hystrix library is designed to do this job. It helps you to control the interactions between these distributed services by adding latency tolerance and fault tolerance logic. Hystrix does this by isolating points of access between the services, remote system, and 3rd party libraries.",
    "explanation": "The context mentions the use of the Hystrix library, which is a well-known implementation of the circuit breaker pattern. It is used to add latency tolerance and fault tolerance logic by isolating points of access between services.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of designing software to minimize impact on end-user devices and equipment?",
    "context": "Treat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nAn outbound filter is used for zipping the content, calculating the metrics, or adding/removing custom headers. After that, the response is sent back to the Netty server and then it is received by the client.\nAdvantages of using ZUUL:\n\n---\n\nThese replicas require a lot of transcoding and preprocessing. Netflix breaks the original video into different smaller chunks and using parallel workers in AWS it converts these chunks into different formats (like mp4, 3gp, etc) across different resolutions (like 4k, 1080p, and more). After transcoding, once we have multiple copies of the files for the same movie, these files are transferred to each and every Open Connect server which is placed in different locations across the world.\n\nBelow is the step by step process of how Netflix ensures optimal streaming quality:\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nNow, there is a good chance that the other person will also have a similar pattern and he/she will do the same thing that the first person has done.\nContent-based filtering:\nThe idea is to filter those videos which are similar to the video a user has liked before.\nContent-based filtering is highly dependent on the information from the products such as movie title, release year, actors, the genre.\nSo to implement this filtering itâ€™s important to know the information describing each item and some sort of user profile describing what the user likes is also desirable.\n4. Database Design of Netflix System Design\nNetflix uses two different databases i.e. MySQL(RDBMS) and Cassandra(NoSQL) for different purposes.",
    "explanation": "The context does not mention any efforts or strategies focused on minimizing the impact of software on end-user devices and equipment.",
    "judgement": "No"
},
{
    "query": "Is there any mention of choosing server regions that are closest to users?",
    "context": "So if youâ€™re trying to play the video sitting in North America, the video will be served from the nearest open connect (or server) instead of the original server (faster response from the nearest server).",
    "explanation": "The context mentions serving video from the nearest open connect server instead of the original server, indicating a strategy to choose server regions closest to users for faster response.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of terminating TLS at the border gateway?",
    "context": "v-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.\n\n---\n\n3.2. How Netflix balance the high traffic load\n1. Elastic Load Balancer\nelastic-load-balancer\n\nELB in Netflix is responsible for routing the traffic to front-end services. ELB performs a two-tier load-balancing scheme where the load is balanced over zones first and then instances (servers).\n\n---\n\nIt includes a lot of powerful and flexible toolkits to display, monitor, and analyze the result.\nChukwe collects the events from different parts of the system and from Chukwe you can do monitoring and analysis or you can use the dashboard to view the events.\nChukwe writes the event in the Hadoop file sequence format (S3). After that Big Data team processes these S3 Hadoop files and writes Hive in Parquet data format.\nThis process is called batch processing which basically scans the whole data at the hourly or daily frequency.\nTo upload online events to EMR/S3, Chukwa also provide traffic to Kafka (the main gate in real-time data processing).\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nmysql\n\nAll the read queries are redirected to the read replicas and only the write queries are redirected to the master nodes.\n\nIn the case of a primary master MySQL failure, the secondary master node will take over the primary role, and the route53 (DNS configuration) entry for the database will be changed to this new primary node.\nThis will also redirect the write queries to this new primary master node.\n4.2. Cassandra\nCassandra is a NoSQL database that can handle large amounts of data and it can also handle heavy writing and reading. When Netflix started acquiring more users, the viewing history data for each member also started increasing. This increases the total number of viewing history data and it becomes challenging for Netflix to handle this massive amount of data.",
    "explanation": "The context does not mention terminating TLS at the border gateway or any related concept.",
    "judgement": "No"
},
{
    "query": "Is there any mention of minimizing the total number of deployed environments?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\n3.3 Supply Service And How it Works? In our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.\n\n---\n\nEarth has a spherical shape so itâ€™s difficult to do summarization and approximation by using latitude and longitude. To solve this problem Uber uses the Google S2 library. This library divides the map data into tiny cells (for example 3km) and gives a unique ID to each cell. This is an easy way to spread data in the distributed system and store it easily. S2 library gives coverage for any given shape easily. Suppose you want to figure out all the supplies available within a 3km radius of a city. Using the S2 libraries you can draw a circle of 3km radius and it will filter out all the cells with IDs that lie in that particular circle. This way you can easily match the rider to the driver and you can easily find out the number of cars(supply) available in a particular region.\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?",
    "explanation": "There is no mention of minimizing the total number of deployed environments.",
    "judgement": "No"
},
{
    "query": "Is there any mention of optimizing average CPU utilization?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\nIt maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?\n\n---\n\n3.3 Supply Service And How it Works? In our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.",
    "explanation": "There is no specific mention of optimizing average CPU utilization to ensure efficient use of resources.",
    "judgement": "No"
},
{
    "query": "Is there any mention of managing peak CPU utilization?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\nIt maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed.\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.)\n\nDemand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs.\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\nIn our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.",
    "explanation": "There is no specific mention of managing peak CPU utilization to avoid over-provisioning.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of removing unused assets?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\nIn our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.)\n\nDemand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs.\n\nWe can represent the entire road network on a graph to calculate the ETAs. We can use AI-simulated algorithms or simple Dijkstraâ€™s algorithm to find out the best route in this graph. In that graph, nodes represent intersections (available cabs), and edges represent road segments. We represent the road segment distance or the traveling time through the edge weight. We also represent and model some additional factors in our graph such as one-way streets, turn costs, turn restrictions, and speed limits. Once the data structure is decided we can find the best route using Dijkstraâ€™s search algorithm which is one of the best modern routing algorithms today. For faster performance, we also need to use OSRM (Open Source Routing Machine) which is based on contraction hierarchies.",
    "explanation": "There is no specific mention of removing unused assets to optimize performance.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of scaling down Kubernetes applications when not in use?",
    "context": "It maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?\n\n---\n\n3.3 Supply Service And How it Works? In our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?\n\n---\n\nETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\n3.6 How To Scale Dispatch System? The dispatch system (including supply, demand, and web socket) is built on NodeJS. NodeJS is the asynchronous and event-based framework that allows you to send and receive messages through WebSockets whenever you want.\n\nUber uses an open-source ringpop to make the application cooperative and scalable for heavy traffic. Ringpop has mainly three parts and it performs the below operation to scale the dispatch system.",
    "explanation": "There is no mention of using Kubernetes in the system.",
    "judgement": "Not Applicable"
  },
  {
    "query": "Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nIt maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?\n\n---\n\n3.3 Supply Service And How it Works? In our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.",
    "explanation": "There is no mention of Kubernetes workloads in the context.",
    "judgement": "Not Applicable"
  },
  {
    "query": "Is there any mention of setting storage retention policies?",
    "context": "Supply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\nETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nGrade A: This subregion is responsible to cover the urban centers and commute areas. Around 90% of Uber traffic gets covered in this subregion, so itâ€™s important to build the highest quality map for subregion A. Grade B: This subregion covers the rural and suburban areas which are less populated and less traveled by Uber customers. Grade AB: A union of grade A and B subregions. Grade C: Covers the set of highway corridors connecting various Uber Territories. 3.8 How Does Uber Build the Map? Uber uses a third-party map service provider to build the map in their application. Earlier Uber was using Mapbox services but later Uber switched to Google Maps API to track the location and calculate ETAs.\n\n---\n\nThe database should be horizontally scalable. You can linearly add capacity by adding more servers. It should be able to handle a lot of reads and writes because once every 4-second cabs will be sending the GPS location and that location will be updated in the database. The system should never give downtime for any operation. It should be highly available no matter what operation you perform (expanding storage, backup, when new nodes are added, etc). Earlier Uber was using the RDBMS PostgreSQL database but due to scalability issues uber switched to various databases. Uber uses a NoSQL database (schemaless) built on top of the MySQL database.\n\n---\n\nIt maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?",
    "explanation": "There is no mention of setting storage retention policies.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of implementing circuit breaker patterns?",
    "context": "It maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?\n\n---\n\nETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\n3.3 Supply Service And How it Works? In our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.",
    "explanation": "There is no mention of implementing circuit breaker patterns.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of regularly scanning for and fixing vulnerabilities?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\n3.3 Supply Service And How it Works? In our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.\n\n---\n\nWe can represent the entire road network on a graph to calculate the ETAs. We can use AI-simulated algorithms or simple Dijkstraâ€™s algorithm to find out the best route in this graph. In that graph, nodes represent intersections (available cabs), and edges represent road segments. We represent the road segment distance or the traveling time through the edge weight. We also represent and model some additional factors in our graph such as one-way streets, turn costs, turn restrictions, and speed limits. Once the data structure is decided we can find the best route using Dijkstraâ€™s search algorithm which is one of the best modern routing algorithms today. For faster performance, we also need to use OSRM (Open Source Routing Machine) which is based on contraction hierarchies.\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?",
    "explanation": "There is no mention of regularly scanning for and fixing vulnerabilities.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of containerizing workloads?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\nThe database should be horizontally scalable. You can linearly add capacity by adding more servers. It should be able to handle a lot of reads and writes because once every 4-second cabs will be sending the GPS location and that location will be updated in the database. The system should never give downtime for any operation. It should be highly available no matter what operation you perform (expanding storage, backup, when new nodes are added, etc). Earlier Uber was using the RDBMS PostgreSQL database but due to scalability issues uber switched to various databases. Uber uses a NoSQL database (schemaless) built on top of the MySQL database.\n\n---\n\nGrade A: This subregion is responsible to cover the urban centers and commute areas. Around 90% of Uber traffic gets covered in this subregion, so itâ€™s important to build the highest quality map for subregion A. Grade B: This subregion covers the rural and suburban areas which are less populated and less traveled by Uber customers. Grade AB: A union of grade A and B subregions. Grade C: Covers the set of highway corridors connecting various Uber Territories. 3.8 How Does Uber Build the Map? Uber uses a third-party map service provider to build the map in their application. Earlier Uber was using Mapbox services but later Uber switched to Google Maps API to track the location and calculate ETAs.\n\n---\n\nIt maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?",
    "explanation": "There is no mention of containerizing workloads.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of deleting unused storage resources?",
    "context": "Supply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\nThe database should be horizontally scalable. You can linearly add capacity by adding more servers. It should be able to handle a lot of reads and writes because once every 4-second cabs will be sending the GPS location and that location will be updated in the database. The system should never give downtime for any operation. It should be highly available no matter what operation you perform (expanding storage, backup, when new nodes are added, etc). Earlier Uber was using the RDBMS PostgreSQL database but due to scalability issues uber switched to various databases. Uber uses a NoSQL database (schemaless) built on top of the MySQL database.\n\n---\n\nGrade A: This subregion is responsible to cover the urban centers and commute areas. Around 90% of Uber traffic gets covered in this subregion, so itâ€™s important to build the highest quality map for subregion A. Grade B: This subregion covers the rural and suburban areas which are less populated and less traveled by Uber customers. Grade AB: A union of grade A and B subregions. Grade C: Covers the set of highway corridors connecting various Uber Territories. 3.8 How Does Uber Build the Map? Uber uses a third-party map service provider to build the map in their application. Earlier Uber was using Mapbox services but later Uber switched to Google Maps API to track the location and calculate ETAs.\n\n---\n\nETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\n3.7 How Does Uber Defines a Map Region? Before launching a new operation in a new area, Uber onboarded the new region to the map technology stack. In this map region, we define various subregions labeled with grades A, B, AB, and C.",
    "explanation": "There is no mention of deleting unused storage resources.",
    "judgement": "No"
  },
  {
    "query":"Is there any mention of caching static data?",
    "context": "Redis for both caching and queuing. Some are behind Twemproxy (which provides scalability of the caching layer). Some are behind a custom clustering system.",
    "explanation":"The context mentions using Redis for caching, including location data, which helps reduce the load on the servers by storing frequently accessed data locally.",
    "judgement":"Yes"
},
{
    "query": "Is there any mention of terminating TLS at the border gateway?",
    "context": "Supply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\nETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nIt maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?\n\n---\n\nTrack HTTP APIs Manage profile Collect feedback and ratings Promotion and coupons etc Fraud detection Payment fraud Incentive abuse by a driver Compromised accounts by hackers. Uber uses historical data of the customer and some machine learning techniques to tackle this problem. 4.5 How To Handle The Data center Failure? Datacenter failure doesnâ€™t happen very often but Uber still maintains a backup data center to run the trip smoothly. This data center includes all the components but Uber never copies the existing data into the backup data center.\n\nThen how does Uber tackle the data center failure??",
    "explanation": "There is no mention of the use of TLS in the system.",
    "judgement": "Not Applicable"
  },
  {
    "query": "Is there any mention of matching service level objectives to business needs?",
    "context": "ETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?\n\n---\n\nIt maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\n3.3 Supply Service And How it Works? In our case, cabs are the supply services and they will be tracked by geolocation (latitude and longitude). All the active cabs keep on sending the location to the server once every 4 seconds through a web application firewall and load balancer. The accurate GPS location is sent to the data center through Kafkaâ€™s Rest APIs once it passes through the load balancer. Here we use Apache Kafka as the data hub. Once the latest location is updated by Kafka it slowly passes through the respective worker notesâ€™ main memory. Also, a copy of the location (state machine/latest location of cabs) will be sent to the database and to the dispatch optimization to keep the latest location updated.",
    "explanation": "There is no mention of matching service level objectives to business needs.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of optimizing the size of AI/ML models?",
    "context": "It maintains consistent hashing to assign the work across the workers. It helps in sharding the application in a way thatâ€™s scalable and fault-tolerant. Ringpop uses RPC (Remote Procedure Call) protocol to make calls from one server to another server. Ringpop also uses a SWIM membership protocol/gossip protocol that allows independent workers to discover each otherâ€™s responsibilities. This way each server/node knows the responsibility and the work of other nodes. Ringpop detects the newly added nodes to the cluster and the node which is removed from the cluster. It distributes the loads evenly when a node is added or removed. 3.7 How Does Uber Defines a Map Region?\n\n---\n\nETA is calculated based on the road system (not geographically) and there are a lot of factors involved in computing the ETA (like heavy traffic or road construction). When a rider requests a cab from a location the app not only identifies the free/idle cabs but also includes the cabs which are about to finish a ride. It may be possible that one of the cabs which are about to finish the ride is closer to the demand than the cab which is far away from the user. So many Uber cars on the road send GPS locations every 4 seconds, so to predict traffic we can use the driverâ€™s appâ€™s GPS location data.\n\n---\n\nWe also need to track a few more things such as the number of seats, the presence of a car seat for children, the type of vehicle, can a wheelchair be fit, and allocation (for example, a cab may have four seats but two of those are occupied.) 3.4 Demand Service And How it Works? Demand service receives the request of the cab through a web socket and it tracks the GPS location of the user. It also receives different kinds of requirements such as the number of seats, type of car, or pool car. Demand gives the location (cell ID) and user requirement to supply and make requests for the cabs. 3.5 How Dispatch System Match the Riders to Drivers?\n\n---\n\nSupply sends the request to the specific server based on the GPS location data. After that, the system draws the circle and filters out all the nearby cabs which meet the riderâ€™s requirements. After that, the list of the cab is sent to the ETA to calculate the distance between the rider and the cab, not geographically but by the road system. The sorted ETA is then sent back to the supply system to offer to a driver. If we need to handle the traffic for the newly added city then we can increase the number of servers and allocate the responsibilities of newly added citiesâ€™ cell IDs to these servers.\n\n---\n\nEarth has a spherical shape so itâ€™s difficult to do summarization and approximation by using latitude and longitude. To solve this problem Uber uses the Google S2 library. This library divides the map data into tiny cells (for example 3km) and gives a unique ID to each cell. This is an easy way to spread data in the distributed system and store it easily. S2 library gives coverage for any given shape easily. Suppose you want to figure out all the supplies available within a 3km radius of a city. Using the S2 libraries you can draw a circle of 3km radius and it will filter out all the cells with IDs that lie in that particular circle. This way you can easily match the rider to the driver and you can easily find out the number of cars(supply) available in a particular region.",
    "explanation": "While there is mention of using machine learning techniques for addressing compromised accounts, there is no specific mention of optimizing the size of AI/ML models to save storage space or reduce memory usage during inference.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of scaling down Kubernetes applications when not in use?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nThese requirements has been covered:\nScalability: We can add more servers to application service layers to make the scalability better and handle numerous requests from the clients. We can also increase the number of databases to store the growing usersâ€™ data.\nLatency: The use of cache and CDNs have reduced the content fetching time.\nAvailability: We have made the system available to the users by using the storage and databases that are replicated across the globe.\nDurability: We have persistent storage that maintains the backup of the data so any uploaded content (photos and videos) never gets lost.\nConsistency: We have used storage like blob stores and databases to keep our data consistent globally.\n\n---\n\nPost photos and videos: The users can post photos and videos on Instagram.\nFollow and unfollow users: The users can follow and unfollow other users on Instagram.\nLike or dislike posts: The users can like or dislike posts of the accounts they follow.\nSearch photos and videos: The users can search photos and videos based on captions and location.\nGenerate news feed: The users can view the news feed consisting of the photos and videos (in chronological order) from all the users they follow.\n2.2 Non-Functional requirements for Instagram System Design\nScalability: The system should be scalable to handle millions of users in terms of computational resources and storage.\nLatency: The latency to generate a news feed should be low.\nAvailability: The system should be highly available.",
    "explanation": "The context does not mention Kubernetes or any specific practices related to scaling down applications when not in use.",
    "judgement": "Not Applicable"
  },
  {
    "query": "Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nPost photos and videos: The users can post photos and videos on Instagram.\nFollow and unfollow users: The users can follow and unfollow other users on Instagram.\nLike or dislike posts: The users can like or dislike posts of the accounts they follow.\nSearch photos and videos: The users can search photos and videos based on captions and location.\nGenerate news feed: The users can view the news feed consisting of the photos and videos (in chronological order) from all the users they follow.\n2.2 Non-Functional requirements for Instagram System Design\nScalability: The system should be scalable to handle millions of users in terms of computational resources and storage.\nLatency: The latency to generate a news feed should be low.\nAvailability: The system should be highly available.\n\n---\n\nurl = \"https://photoslibrary.googleapis.com/v1/mediaItems:search\"\nheaders = {\n\"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"  # Replace with your access token\n}\ndata = {\n\"filters\": {\n\"mediaTypes\": [\"PHOTO\", \"VIDEO\"]  # Search for both photos and videos\n}\n}\nresponse = requests.post(url, headers=headers, json=data)\n \nif response.status_code == 200:\nresults = response.json()\n# Process the search results\nelse:\nprint(\"Error:\", response.status_code, response.text)\n8. Database Design for Instagram Database Design\nWe need the following tables to store our data:",
    "explanation": "The context does not discuss Kubernetes or the use of metrics to adjust workloads dynamically",
    "judgement": "Not Applicable"
  },
  {
    "query": "Is there any mention of allowing independent scaling of logical components?",
    "context": "Image and Feed generation service used as microservice architecture.\n\nMicroservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.",
    "explanation": "The system design includes microservices architecture which inherently supports the independent scaling of logical components.",
    "judgement": "Yes"
  },
  {
    "query": "Is there any mention of adopting serverless cloud services?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\n3.1 Storage Per Day\nPhotos: 60 million photos/day * 3 MB = 180 TeraBytes / day\nVideos: 35 million videos/day * 150 MB = 5250 TB / day\nTotal content size = 180 + 5250 = 5430 TB\nThe Total Space required for a Year:\n5430 TB/day * 365 (days a year) = 1981950 TB = 1981.95 PetaBytes\n\n3.2 Bandwidth Estimation\n5430 TB/(24 * 60* 60) = 5430 TB/86400 sec ~= 62.84 GB/s ~= 502.8 Gbps\nIncoming bandwidth ~= 502.8 Gbps\nLetâ€™s say the ratio of readers to writers is 100:1.\nRequired outgoing bandwidth ~= 100 * 502.8 Gbps ~= 50.28 Tbps\n\n4. Use Case Diagram for Instagram System Design\numl\nUse Case Diagram Instagram\n\nIn the above Diagram we have discussed about the use case diagram of Instagram:\n\n---\n\nFeed Generation adjusts based on changed relationships.\nUser monitors activity:\nClient checks notifications feed.\nNotifications provide updates on relevant events.\nKey Design Considerations:",
    "explanation": "There is no mention of adopting serverless technologies within the context.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of reducing transmitted data?",
    "context": "Notification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\n2. Requirements for Instagram System Design\n2.1 Functional Requirements for Instagram System Design\nIn functional Requirements, we will not discuss the login or signup page of Instagram. Login and Signup architecture is the same for everyone. We will look for further like posting photos, etc.\n\n---\n\n{\nuserId: string[Hashkey]\nuploadId: string\ncreationDateInUtc: long[RangeKey]\n}\n8.5 Which Database we should select for Data storing?\nIt is essential to choose the right kind of database for our Instagram system, but which is the right choice â€” SQL or NoSQL? Our data is inherently relational, and we need an order for the data (posts should appear in chronological order) and no data loss even in case of failures (data durability). Moreover, in our case, we would benefit from relational queries like fetching the followers or images based on a user ID. Hence, SQL-based databases fulfill these requirements.\n\nImage and Feed generation service used as microservice architecture.\n\n9. Microservices for Instagram System Design\nImage and Feed generation service used as microservice architecture.\n\n---\n\n10. Scalability for Instagram System Design\nScalability refers to the ability of an organization (or a system, such as a computer network) to perform well under an increased or expanding workload. A system that scales well will be able to maintain or increase its level of performance even as it is tested by larger and larger operational demands.\n\nWe can add more servers to application service layers to make the scalability better and handle numerous requests from the clients. We can also increase the number of databases to store the growing usersâ€™ data.\n\n---\n\nMicroservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.",
    "explanation": "The context does not discuss strategies for reducing transmitted data as a means to save energy.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of scheduling Kubernetes cron jobs during off-peak hours?",
    "context": "API Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\nMicroservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nThese requirements has been covered:\nScalability: We can add more servers to application service layers to make the scalability better and handle numerous requests from the clients. We can also increase the number of databases to store the growing usersâ€™ data.\nLatency: The use of cache and CDNs have reduced the content fetching time.\nAvailability: We have made the system available to the users by using the storage and databases that are replicated across the globe.\nDurability: We have persistent storage that maintains the backup of the data so any uploaded content (photos and videos) never gets lost.\nConsistency: We have used storage like blob stores and databases to keep our data consistent globally.\n\n---\n\nurl = \"https://photoslibrary.googleapis.com/v1/mediaItems:search\"\nheaders = {\n\"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"  # Replace with your access token\n}\ndata = {\n\"filters\": {\n\"mediaTypes\": [\"PHOTO\", \"VIDEO\"]  # Search for both photos and videos\n}\n}\nresponse = requests.post(url, headers=headers, json=data)\n \nif response.status_code == 200:\nresults = response.json()\n# Process the search results\nelse:\nprint(\"Error:\", response.status_code, response.text)\n8. Database Design for Instagram Database Design\nWe need the following tables to store our data:",
    "explanation": "The context does not mention Kubernetes or specific scheduling practices like cron jobs during off-peak hours.",
    "judgement": "Not Applicable"
  },
  {
    "query": "Is there any mention of adopting cloud-native network security tools and controls?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nFeed Generation adjusts based on changed relationships.\nUser monitors activity:\nClient checks notifications feed.\nNotifications provide updates on relevant events.\nKey Design Considerations:\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\n3.1 Storage Per Day\nPhotos: 60 million photos/day * 3 MB = 180 TeraBytes / day\nVideos: 35 million videos/day * 150 MB = 5250 TB / day\nTotal content size = 180 + 5250 = 5430 TB\nThe Total Space required for a Year:\n5430 TB/day * 365 (days a year) = 1981950 TB = 1981.95 PetaBytes\n\n3.2 Bandwidth Estimation\n5430 TB/(24 * 60* 60) = 5430 TB/86400 sec ~= 62.84 GB/s ~= 502.8 Gbps\nIncoming bandwidth ~= 502.8 Gbps\nLetâ€™s say the ratio of readers to writers is 100:1.\nRequired outgoing bandwidth ~= 100 * 502.8 Gbps ~= 50.28 Tbps\n\n4. Use Case Diagram for Instagram System Design\numl\nUse Case Diagram Instagram\n\nIn the above Diagram we have discussed about the use case diagram of Instagram:",
    "explanation": "While the context discusses robust security measures, it does not specify the adoption of cloud-native network security tools and controls.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of choosing server regions that are closest to users?",
    "context": "Notification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nMicroservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nPost photos and videos: The users can post photos and videos on Instagram.\nFollow and unfollow users: The users can follow and unfollow other users on Instagram.\nLike or dislike posts: The users can like or dislike posts of the accounts they follow.\nSearch photos and videos: The users can search photos and videos based on captions and location.\nGenerate news feed: The users can view the news feed consisting of the photos and videos (in chronological order) from all the users they follow.\n2.2 Non-Functional requirements for Instagram System Design\nScalability: The system should be scalable to handle millions of users in terms of computational resources and storage.\nLatency: The latency to generate a news feed should be low.\nAvailability: The system should be highly available.\n\n---\n\nThese requirements has been covered:\nScalability: We can add more servers to application service layers to make the scalability better and handle numerous requests from the clients. We can also increase the number of databases to store the growing usersâ€™ data.\nLatency: The use of cache and CDNs have reduced the content fetching time.\nAvailability: We have made the system available to the users by using the storage and databases that are replicated across the globe.\nDurability: We have persistent storage that maintains the backup of the data so any uploaded content (photos and videos) never gets lost.\nConsistency: We have used storage like blob stores and databases to keep our data consistent globally.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).",
    "explanation": "The context does not discuss the strategy of selecting server regions closest to users for reducing latency and energy consumption.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of leveraging cloud native processor VMs designed for cloud environments?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\n3.1 Storage Per Day\nPhotos: 60 million photos/day * 3 MB = 180 TeraBytes / day\nVideos: 35 million videos/day * 150 MB = 5250 TB / day\nTotal content size = 180 + 5250 = 5430 TB\nThe Total Space required for a Year:\n5430 TB/day * 365 (days a year) = 1981950 TB = 1981.95 PetaBytes\n\n3.2 Bandwidth Estimation\n5430 TB/(24 * 60* 60) = 5430 TB/86400 sec ~= 62.84 GB/s ~= 502.8 Gbps\nIncoming bandwidth ~= 502.8 Gbps\nLetâ€™s say the ratio of readers to writers is 100:1.\nRequired outgoing bandwidth ~= 100 * 502.8 Gbps ~= 50.28 Tbps\n\n4. Use Case Diagram for Instagram System Design\numl\nUse Case Diagram Instagram\n\nIn the above Diagram we have discussed about the use case diagram of Instagram:\n\n---\n\nFeed Generation adjusts based on changed relationships.\nUser monitors activity:\nClient checks notifications feed.\nNotifications provide updates on relevant events.\nKey Design Considerations:",
    "explanation": "The context does not mention the use of cloud native processor VMs specifically designed for cloud environments.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of terminating TLS at the border gateway ?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\nPost photos and videos: The users can post photos and videos on Instagram.\nFollow and unfollow users: The users can follow and unfollow other users on Instagram.\nLike or dislike posts: The users can like or dislike posts of the accounts they follow.\nSearch photos and videos: The users can search photos and videos based on captions and location.\nGenerate news feed: The users can view the news feed consisting of the photos and videos (in chronological order) from all the users they follow.\n2.2 Non-Functional requirements for Instagram System Design\nScalability: The system should be scalable to handle millions of users in terms of computational resources and storage.\nLatency: The latency to generate a news feed should be low.\nAvailability: The system should be highly available.\n\n---\n\n3.1 Storage Per Day\nPhotos: 60 million photos/day * 3 MB = 180 TeraBytes / day\nVideos: 35 million videos/day * 150 MB = 5250 TB / day\nTotal content size = 180 + 5250 = 5430 TB\nThe Total Space required for a Year:\n5430 TB/day * 365 (days a year) = 1981950 TB = 1981.95 PetaBytes\n\n3.2 Bandwidth Estimation\n5430 TB/(24 * 60* 60) = 5430 TB/86400 sec ~= 62.84 GB/s ~= 502.8 Gbps\nIncoming bandwidth ~= 502.8 Gbps\nLetâ€™s say the ratio of readers to writers is 100:1.\nRequired outgoing bandwidth ~= 100 * 502.8 Gbps ~= 50.28 Tbps\n\n4. Use Case Diagram for Instagram System Design\numl\nUse Case Diagram Instagram\n\nIn the above Diagram we have discussed about the use case diagram of Instagram:",
    "explanation": "The context does not mention the practice of terminating TLS at the border gateway to reduce encryption overhead.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of matching service level objectives to business needs?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\n3.1 Storage Per Day\nPhotos: 60 million photos/day * 3 MB = 180 TeraBytes / day\nVideos: 35 million videos/day * 150 MB = 5250 TB / day\nTotal content size = 180 + 5250 = 5430 TB\nThe Total Space required for a Year:\n5430 TB/day * 365 (days a year) = 1981950 TB = 1981.95 PetaBytes\n\n3.2 Bandwidth Estimation\n5430 TB/(24 * 60* 60) = 5430 TB/86400 sec ~= 62.84 GB/s ~= 502.8 Gbps\nIncoming bandwidth ~= 502.8 Gbps\nLetâ€™s say the ratio of readers to writers is 100:1.\nRequired outgoing bandwidth ~= 100 * 502.8 Gbps ~= 50.28 Tbps\n\n4. Use Case Diagram for Instagram System Design\numl\nUse Case Diagram Instagram\n\nIn the above Diagram we have discussed about the use case diagram of Instagram:\n\n---\n\nFeed Generation adjusts based on changed relationships.\nUser monitors activity:\nClient checks notifications feed.\nNotifications provide updates on relevant events.\nKey Design Considerations:\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).",
    "explanation": "The context does not mention aligning service level objectives with business needs to tailor performance goals.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of adopting a serverless architecture for AI/ML workload processes?",
    "context": "API Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\nMicroservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nThese requirements has been covered:\nScalability: We can add more servers to application service layers to make the scalability better and handle numerous requests from the clients. We can also increase the number of databases to store the growing usersâ€™ data.\nLatency: The use of cache and CDNs have reduced the content fetching time.\nAvailability: We have made the system available to the users by using the storage and databases that are replicated across the globe.\nDurability: We have persistent storage that maintains the backup of the data so any uploaded content (photos and videos) never gets lost.\nConsistency: We have used storage like blob stores and databases to keep our data consistent globally.\n\n---\n\n3.1 Storage Per Day\nPhotos: 60 million photos/day * 3 MB = 180 TeraBytes / day\nVideos: 35 million videos/day * 150 MB = 5250 TB / day\nTotal content size = 180 + 5250 = 5430 TB\nThe Total Space required for a Year:\n5430 TB/day * 365 (days a year) = 1981950 TB = 1981.95 PetaBytes\n\n3.2 Bandwidth Estimation\n5430 TB/(24 * 60* 60) = 5430 TB/86400 sec ~= 62.84 GB/s ~= 502.8 Gbps\nIncoming bandwidth ~= 502.8 Gbps\nLetâ€™s say the ratio of readers to writers is 100:1.\nRequired outgoing bandwidth ~= 100 * 502.8 Gbps ~= 50.28 Tbps\n\n4. Use Case Diagram for Instagram System Design\numl\nUse Case Diagram Instagram\n\nIn the above Diagram we have discussed about the use case diagram of Instagram:\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.",
    "explanation": "There is no mention of adopting serverless architectures specifically for AI/ML workloads in the context.",
    "judgement": "No"
  },
  {
    "query": "Is there any mention of using efficient file formats like Parquet or any other efficient format?",
    "context": "Microservices â€“ also known as the microservice architecture â€“ is an architectural style that structures an application as a collection of services that are:\n\nIndependently deployable\nLoosely coupled\nOrganized around business capabilities\nOwned by a small team\nThe microservice architecture enables an organization to deliver large, complex applications rapidly, frequently, reliably and sustainably â€“ a necessity for competing and winning in todayâ€™s world.\n\n---\n\nAPI Endpoints:\n\nGET /posts/{post_id}: Retrieves details of a post, including the number of likes/dislikes.\nPOST /posts/{post_id}/like: Registers a like for the post by the authenticated user.\nPOST /posts/{post_id}/dislike: Registers a dislike for the post by the authenticated user.\nDELETE /posts/{post_id}/like: Removes the like for the post by the authenticated user (if previously done).\nDELETE /posts/{post_id}/dislike: Removes the dislike for the post by the authenticated user (if previously done).\n\n---\n\nNotification Service:\nInforms users about relevant events like likes, comments, mentions, and follows.\nPushes notifications to mobile devices through platforms like Firebase Cloud Messaging or Amazon SNS.\nLeverages a queueing system for asynchronous notification delivery.\nAnalytics Service:\nTracks user engagement, post performance, and overall platform usage.\nGathers data on views, likes, comments, shares, and clicks.\nProvides insights to improve user experience, optimize content recommendations, and target advertising.\nWhy we need caching for storing the data?\nCache the data to handle millions of reads. It improves the user experience by making the fetching process fast. Weâ€™ll also opt for lazy loading, which minimizes the clientâ€™s waiting time.\n\n---\n\nThese requirements has been covered:\nScalability: We can add more servers to application service layers to make the scalability better and handle numerous requests from the clients. We can also increase the number of databases to store the growing usersâ€™ data.\nLatency: The use of cache and CDNs have reduced the content fetching time.\nAvailability: We have made the system available to the users by using the storage and databases that are replicated across the globe.\nDurability: We have persistent storage that maintains the backup of the data so any uploaded content (photos and videos) never gets lost.\nConsistency: We have used storage like blob stores and databases to keep our data consistent globally.\n\n---\n\n3.1 Storage Per Day\nPhotos: 60 million photos/day * 3 MB = 180 TeraBytes / day\nVideos: 35 million videos/day * 150 MB = 5250 TB / day\nTotal content size = 180 + 5250 = 5430 TB\nThe Total Space required for a Year:\n5430 TB/day * 365 (days a year) = 1981950 TB = 1981.95 PetaBytes\n\n3.2 Bandwidth Estimation\n5430 TB/(24 * 60* 60) = 5430 TB/86400 sec ~= 62.84 GB/s ~= 502.8 Gbps\nIncoming bandwidth ~= 502.8 Gbps\nLetâ€™s say the ratio of readers to writers is 100:1.\nRequired outgoing bandwidth ~= 100 * 502.8 Gbps ~= 50.28 Tbps\n\n4. Use Case Diagram for Instagram System Design\numl\nUse Case Diagram Instagram\n\nIn the above Diagram we have discussed about the use case diagram of Instagram:",
    "explanation": "The context does not mention the use of efficient file formats like Parquet or any specific data management practices for AI/ML development",
    "judgement": "No"
  }
]

