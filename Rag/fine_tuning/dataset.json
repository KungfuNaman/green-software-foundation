[{
    "query": "Is there any mention of allowing independent scaling of logical components?",
    "context": "This combination of edge wrapper and ORM simplifies database management, making it easier to scale the application horizontally with sharded databases while maintaining a cohesive and developer-friendly interface.",
    "explanation": "The context mentions the combination of edge wrapper and ORM simplifying database management, which facilitates horizontal scaling with sharded databases. This indicates that independent scaling of logical components is supported as the system can add capacity by adding more servers without disrupting the existing setup.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of queuing non-urgent processing requests?",
    "context": "The messaging service queue will be responsible for the asynchronous communication between the clients and the synchronization service.",
    "explanation": "The context mentions that the messaging service queue handles asynchronous communication between clients and the synchronization service, which implies that non-urgent processing requests are queued to ensure efficient handling.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of using asynchronous network calls instead of synchronous ones?",
    "context": "The messaging service queue will be responsible for the asynchronous communication between the clients and the synchronization service.",
    "explanation": "From the context, it can be derived that asynchronous network calls are used for communication between clients and the synchronization service, indicating the use of asynchronous calls.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of adopting cloud-native network security tools and controls?",
    "context": "The client application requests a Presigned URL from the Upload Service. The server generates the Presigned URL by interacting with the S3 service, creating a unique token for the specific upload operation. These URLs grant temporary, secure access to upload a specific file to a designated S3 bucket.",
    "explanation": "The context mentions the use of Presigned URLs for secure file uploads to an S3 bucket. This approach involves generating unique, time-limited tokens that provide secure access to cloud storage resources, indicating the adoption of cloud-native security controls to safeguard data during upload operations.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of implementing a stateless design?",
    "context": "Request Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\ndesign-dropbox-2\n\nLet\u2019s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it is received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  new_objects: [\n    {\n      object_id:\n      object_type:\n      name:\n      chunk_ids: [\n        chunk1,\n        chunk2,\n        chunk3\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing the name of the object, object id, object type, and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.",
    "explanation": "The context does not discuss implementing a stateless design for scalability.",
    "judgement": "No"
},
{
    "query": "Is there any mention of scaling down Kubernetes applications when not in use?",
    "context": "Request Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\ndesign-dropbox-2\n\nLetâ€™s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nYou just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.",
    "explanation": "The context does not mention the use of Kubernetes in the system, and therefore, considering whether Kubernetes applications are scaled down when not in use is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling infrastructure based on real-time user demand?",
    "context": "Request Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\ndesign-dropbox-2\n\nLet\u2019s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it is received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  new_objects: [\n    {\n      object_id:\n      object_type:\n      name:\n      chunk_ids: [\n        chunk1,\n        chunk2,\n        chunk3\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing the name of the object, object id, object type, and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.",
    "explanation": "There is no mention in the context of scaling infrastructure based on real-time user demand.",
    "judgement": "No"
},
{
    "query": "Is there any mention of optimizing the size of AI/ML models?",
    "context": "We can break the files into multiple chunks to overcome the problem we discussed above. There is no need to upload/download the whole single file after making any changes in the file.\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it will be received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.\n\n---\n\nYou just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.\n\n---\n\nRequest Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).",
    "explanation": "Artifical Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of leveraging pre-trained models and transfer learning?",
    "context": "design-dropbox-2\n\nLetâ€™s assume we have a client installed on our computer (an app installed on your computer) and this client has 4 basic components. These basic components are Watcher, Chunker, Indexer, and Internal DB. We have considered only one client but there can be multiple clients belonging to the same user with the same basic components.\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\nRequest Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\nIt gives a notification to the indexer and chunker if any action is performed in the files or folders.\nChunker breaks the files into multiple small pieces called chunks and uploads them to the cloud storage with a unique id or hash of these chunks.\nTo recreate the files these chunks can be joined together. For any changes in the files, the chunking algorithm detects the specific chunk which is modified and only saves that specific part/chunk to the cloud storage.\nIt reduces bandwidth usage, synchronization time, and storage space in the cloud.\nIndexer is responsible for updating the internal database when it receives the notification from the watcher (for any action performed in folders/files).\n\n---\n\nYou just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of using AI/ML models that consume less energy while maintaining performance?",
    "context": "You just need to save the chunk which is updated (this will take less memory and time). It will be easier to keep the different versions of the files in various chunks.\nWe have considered one file which is divided into various chunks. If there are multiple files then we need to know which chunks belong to which file.\nTo keep this information we will create one more file named a metadata file. This file contains the indexes of the chunks (chunk names and order information).\nYou need to mention the hash of the chunks (or some reference) in this metadata file and you need to sync this file into the cloud. We can download the metadata file from the cloud whenever we want and we can recreate the file using various chunks.\n\n---\n\nRequest Queue:\nThis will be a global request queue shared among all the clients.\nWhenever a client receives any update or changes in the files/folder it sends the request through the request queue.\nThis request is received by the synchronization service to update the metadata database.\nResponse Queue:\nThere will be an individual response queue corresponding to the individual clients.\nThe synchronization service broadcasts the update through this response queue and this response queue will deliver the updated messages to each client and then these clients will update their respective files accordingly.\nThe message will never be lost even if the client is disconnected from the internet (the benefit of using the messaging queue service).\n\n---\n\nRequest\n\nGET /api/v1/objects?local_object_id=<Max object_id present locally>&device_id=<Unique Device Id>\nX-API-Key: api_key\nAuthorization: auth_token\nResponse\n\n200 OK\n{\n  \"new_objects\": [\n    {\n      \"object_id\": null,\n      \"object_type\": null,\n      \"name\": null,\n      \"chunk_ids\": [\n        \"chunk1\",\n        \"chunk2\",\n        \"chunk3\"\n      ]\n    }\n  ]\n}\nMeta Service will check the database and return an array of objects containing name of object, object id, object type and an array of chunk_ids. Client calls the Download Chunk API with these chunk_ids to download the chunks and reconstruct the file.\n\n---\n\nNon Functional Requirements:\nAvailability: Availability means what percentage of the time the system is available to take a userâ€™s request. We generally mention availability as 5 Nineâ€™s, 4 Nineâ€™s, etc. 5 Nineâ€™s means 99.999% availability, 4 Nineâ€™s means 99.99% availability, and so on.\nDurability: Durability means the data uploaded by the user should be permanently stored in the database even in case of database failure. Our System should ensure the files uploaded by the user should be permanently stored on the drive without any data loss.\nReliability: Reliability means how many times the system gives the expected output for the same input.\nScalability: With the growing number of users our system should be capable enough to handle the increasing traffic.\n\n---\n\nWe are creating n number of response queues for n number of clients because the message will be deleted from the queue once it will be received by the client and we need to share the updated message with the various subscribed clients.\n4.4. Synchronization Service\nThe client communicates with the synchronization services either to receive the latest update from the cloud storage or to send the latest request/updates to the Cloud Storage.",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling down Kubernetes applications when not in use?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:",
    "explanation": "The context does not mention the use of Kubernetes in the system, and therefore, considering whether Kubernetes applications are scaled down when not in use is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling infrastructure based on real-time user demand?",
    "context": "We will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.",
    "explanation": "The context mentions the use of a microservices architecture to facilitate horizontal scaling and decoupling of services, indicating the ability to scale infrastructure based on real-time user demand by adding or removing instances of services as needed.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?",
    "context": "Download Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nThe responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.",
    "explanation": "The context does not mention the use of Kubernetes in the system and therefore, considering whether relevant metrics are used to dynamically adjust Kubernetes workloads is irrelevant",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scheduling Kubernetes cron jobs during off-peak hours?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.",
    "explanation": "The context does not mention the use of Kubernetes in the system and therefore, considering whether Kubernetes cron jobs are scheduled during off-peak hours is irrelevant.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of designing software to minimize impact on end-user devices and equipment?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nImportant Topics for WhatsApp System Design\n\nRequirements\nCapacity Estimation\nHigh Level Design (HLD) of WhatsApp Messenger\nData Model Design:\nAPI Design\nLow Level Design (LLD) of System Design\nApproach to achieve the below system attributes\nRequirements\nThe WhatsApp messenger design should meet below requirements:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30*10 TB/day = 300 TB/month",
    "explanation": "The context does not mention designing software to minimize impact on end-user devices and equipment.",
    "judgement": "No"
},
{
    "query": "Is there any mention of choosing server regions that are closest to users?",
    "context": "The group service keeps all information about users in each group in the system. It has all the information about each group, including user IDs, group ID, status, group icon, number of users, and so on. This service resides on top of the MySQL database cluster, with multiple secondary replicas distributed geographically. A Redis cache server also exists to cache data from the MySQL servers. Both geographically distributed replicas and Redis cache aid in reducing latency.",
    "explanation": "The context mentions that the service uses geographically distributed replicas of the MySQL database cluster and Redis cache servers to reduce latency. This implies choosing server regions closer to users.",
    "judgement": "Yes"
},
{
    "query":"Is there any mention of deleting unused storage resources?",
    "context":"Message service is a repository of messages on top of the Mnesia database cluster. It acts as an interface to the Mnesia database for other services interacting with the databases. It is responsible for storing and retrieving messages from the Mnesia database. It also deletes messages from the Mnesia database after a configurable amount of time.",
    "explanation":"The context mentions that messages are deleted from the Mnesia database after configurable amount of time",
    "judgement":"Yes"
},
{
    "query":"Is there any mention of caching static data?",
    "context":"A Redis cache server also exists to cache data from the MySQL servers. Both geographically distributed replicas and Redis cache aid in reducing latency.",
    "explanation":"The context mentions the use of Redis, a cache server, which indicates that static data is being cached in the system.",
    "judgement":"Yes"
},
{
    "query":"Is there any mention of compressing stored data?",
    "context":"The media file is compressed and encrypted on the device side. The compressed and encrypted file is sent to the asset service to store the file on blob storage.",
    "explanation":"The context mentions that media files are compressed on the device side before being sent to the asset service for storage. This reduces the amount of data stored, thus improving data efficiency.",
    "judgement":"Yes"
},
{
    "query":"Is there any mention of compressing data transmitted over networks?",
    "context":"The media file is compressed and encrypted on the device side. The compressed and encrypted file is sent to the asset service to store the file on blob storage.",
    "explanation":"The context mentions that media files are compressed on the device side before being transmitted to the asset service, which indicates an effort to minimize the amount of data transmitted over the network.",
    "judgement":"Yes"
},
{
    "query": "Is there any mention of implementing a stateless design?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30âˆ—10 TB/day = 300 TB/month",
    "explanation": "The context does not mention implementing a stateless design for scalability. Implementing a stateless design is relevant for such an application.",
    "judgement": "No"
},
{
    "query": "Is there any mention of matching service level objectives to business needs?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nImportant Topics for WhatsApp System Design\n\nRequirements\nCapacity Estimation\nHigh Level Design (HLD) of WhatsApp Messenger\nData Model Design:\nAPI Design\nLow Level Design (LLD) of System Design\nApproach to achieve the below system attributes\nRequirements\nThe WhatsApp messenger design should meet below requirements:",
    "explanation": "The context does not mention matching service level objectives to business needs. The query is still relevant to the system but is not addressed in the provided context.",
    "judgement": "No"
},
{
    "query": "Is there any mention of deploying AI models on edge devices?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nFunctional Requirement\nConversation: The system should support one-on-one and group conversations between users.\nAcknowledgment: The system should support message delivery acknowledgment, such as sent, delivered, and read.\nSharing: The system should support sharing of media files, such as images, videos, and audio.\nChat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.\nPush notifications: The system should be able to notify offline users of new messages once their status becomes online.\nNon-Functional Requirement\nLow latency: Users should be able to receive messages with low latency.\nConsistency: Messages should be delivered in the order they were sent.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30âˆ—10 TB/day = 300 TB/month",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of adopting a serverless architecture for AI/ML workload processes?",
    "context": "The responsibility of each of these servers is to provide a port to every online user. The mapping between servers, ports, and users is stored in the WebSocket manager that resides on top of a cluster of the data store. In this case, thatâ€™s Redis.\n\n2. Send or receive messages\nThe WebSocket manager is responsible for maintaining a mapping between an active user and a port assigned to the user. Whenever a user is connected to another WebSocket server, this information will be updated in the data store. A WebSocket server also communicates with another service called message service.\n\n---\n\nFor 30 days, the storage capacity would become the following:\n30âˆ—10 TB/day = 300 TB/month\n\n---\n\nDownload Media File\ndownloadFile(user_id, file_id)\n\nArchitecture\nWe will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model.\n\nLow Level Design (LLD) of System Design\nLow-Level-Design(LLD)-of-WhatsApp-Messenger\n\n1. Connection with Websocket Server\nIn WhatsApp, each active device is connected with a WebSocket server via WebSocket protocol. A WebSocket server keeps the connection open with all the active (online) users. Since one server isnâ€™t enough to handle billions of devices, there should be enough servers to handle billions of users.\n\n---\n\nBandwidth Estimation:\nAccording to the storage capacity estimation, our service will get 10TB of data each day, giving us a bandwidth of 926 Mb/s.\n10 TB/86400sec â‰ˆ 926Mb/s\n\nNumber of servers estimation:\nWhatsApp handles around 10 million connections on a single server, which seems quite high for a server.\nNo. of servers = Total connections per day/No. of connections per server = 2 billion/10 million = 200 servers\nSo, according to the above estimates, we require 200 chat servers.\n\nHigh Level Design (HLD) of WhatsApp Messenger\nHigh-Level-Design-(HLD)-of-WhatsApp-Messenger\n\nThe following steps describe the communication between both clients:\n\n---\n\nImportant Topics for WhatsApp System Design\n\nRequirements\nCapacity Estimation\nHigh Level Design (HLD) of WhatsApp Messenger\nData Model Design:\nAPI Design\nLow Level Design (LLD) of System Design\nApproach to achieve the below system attributes\nRequirements\nThe WhatsApp messenger design should meet below requirements:",
    "explanation": "Artificial Intelligence or Machine Learning is not mentioned in the context, and therefore this query is not applicable.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of scaling infrastructure based on real-time user demand?",
    "context": "Treat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nThe data is shared across the cluster within the same zone and multiple copies of the cache are stored in sharded nodes.\nEvery time when write happens to the client all the nodes in all the clusters are updated but when the read happens to the cache, it is only sent to the nearest cluster (not all the cluster and nodes) and its nodes.\nIn case, a node is not available then read from a different available node. This approach increases performance, availability, and reliability.\n3.4. Data Processing in Netflix Using Kafka And Apache Chukwa\nWhen you click on a video Netflix starts processing data in various terms and it takes less than a nanosecond. Letâ€™s discuss how the evolution pipeline works on Netflix.\n\n---\n\nAn outbound filter is used for zipping the content, calculating the metrics, or adding/removing custom headers. After that, the response is sent back to the Netty server and then it is received by the client.\nAdvantages of using ZUUL:\n\n---\n\nmysql\n\nAll the read queries are redirected to the read replicas and only the write queries are redirected to the master nodes.\n\nIn the case of a primary master MySQL failure, the secondary master node will take over the primary role, and the route53 (DNS configuration) entry for the database will be changed to this new primary node.\nThis will also redirect the write queries to this new primary master node.\n4.2. Cassandra\nCassandra is a NoSQL database that can handle large amounts of data and it can also handle heavy writing and reading. When Netflix started acquiring more users, the viewing history data for each member also started increasing. This increases the total number of viewing history data and it becomes challenging for Netflix to handle this massive amount of data.",
    "explanation": "The context does not specifically mention scaling infrastructure based on real-time user demand. While it discusses general scaling and fault tolerance strategies, it does not explicitly cover real-time scaling based on user demand.",
    "judgement": "No"
},
{
    "query": "Is there any mention of using relevant metrics to dynamically adjust Kubernetes workloads?",
    "context": "Netflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nev-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.\n\n---\n\nIt includes a lot of powerful and flexible toolkits to display, monitor, and analyze the result.\nChukwe collects the events from different parts of the system and from Chukwe you can do monitoring and analysis or you can use the dashboard to view the events.\nChukwe writes the event in the Hadoop file sequence format (S3). After that Big Data team processes these S3 Hadoop files and writes Hive in Parquet data format.\nThis process is called batch processing which basically scans the whole data at the hourly or daily frequency.\nTo upload online events to EMR/S3, Chukwa also provide traffic to Kafka (the main gate in real-time data processing).\n\n---\n\nWhen you load the front page you see multiple rows of different kinds of movies. Netflix personalizes this data and decides what kind of rows or what kind of movies should be displayed to a specific user. This data is based on the userâ€™s historical data and preferences.\n\nAlso, for that specific user, Netflix performs sorting of the movies and calculates the relevance ranking (for the recommendation) of these movies available on their platform. In Netflix, Apache Spark is used for content recommendations and personalization.\n\nA majority of the machine learning pipelines are run on these large spark clusters. These pipelines are then used to do row selection, sorting, title relevance ranking, and artwork personalization among others.\n\n---\n\nTreat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?",
    "explanation": "The context does not mention the use of Kubernetes in the system and therefore, considering whether relevant metrics are used to dynamically adjust Kubernetes workloads is irrelevant.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of reducing transmitted data?",
    "context": "This happens because the application keeps checking the best streaming open connect server and switches between formats (for the best viewing experience) when itâ€™s needed. \nUser data is saved in AWS such as searches, viewing, location, device, reviews, and likes, Netflix uses it to build the movie recommendation for users using the Machine learning model or Hadoop.\n\n---\n\n3.2. How Netflix balance the high traffic load\n1. Elastic Load Balancer\nelastic-load-balancer\n\nELB in Netflix is responsible for routing the traffic to front-end services. ELB performs a two-tier load-balancing scheme where the load is balanced over zones first and then instances (servers).\n\n---\n\n2.1. Microservices Architecture of Netflix \nNetflixâ€™s architectural style is built as a collection of services. This is known as microservices architecture and this power all of the APIs needed for applications and Web apps. When the request arrives at the endpoint it calls the other microservices for required data and these microservices can also request the data from different microservices. After that, a complete response for the API request is sent back to the endpoint.\n\n---\n\n3.1. How Does Netflix Onboard a Movie/Video?\nNetflix receives very high-quality videos and content from the production houses, so before serving the videos to the users it does some preprocessing.\n\n---\n\nNetflix scaled the storage of viewing history data-keeping two main goals in their mind:\n\nSmaller Storage Footprint.\nConsistent Read/Write Performance as viewing per member grows (viewing history data write-to-read ratio is about 9:1 in Cassandra).\ncasandra-service-pattern\n\nTotal Denormalized Data Model  \nOver 50 Cassandra Clusters\nOver 500 Nodes\nOver 30TB of daily backups\nThe biggest cluster has 72 nodes.\n1 cluster over 250K writes/s\nInitially, the viewing history was stored in Cassandra in a single row. When the number of users started increasing on Netflix the row sizes as well as the overall data size increased. This resulted in high storage, more operational cost, and slow performance of the application. The solution to this problem was to compress the old rows.",
    "explanation": "The context does not mention any specific methods for reducing transmitted data. While there is information on data storage and processing, it does not address strategies for minimizing the amount of data transmitted over the network.",
    "judgement": "No"
},
{
    "query": "Is there any mention of scheduling Kubernetes cron jobs during off-peak hours?",
    "context": "Treat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nAn outbound filter is used for zipping the content, calculating the metrics, or adding/removing custom headers. After that, the response is sent back to the Netty server and then it is received by the client.\nAdvantages of using ZUUL:\n\n---\n\nThe data is shared across the cluster within the same zone and multiple copies of the cache are stored in sharded nodes.\nEvery time when write happens to the client all the nodes in all the clusters are updated but when the read happens to the cache, it is only sent to the nearest cluster (not all the cluster and nodes) and its nodes.\nIn case, a node is not available then read from a different available node. This approach increases performance, availability, and reliability.\n3.4. Data Processing in Netflix Using Kafka And Apache Chukwa\nWhen you click on a video Netflix starts processing data in various terms and it takes less than a nanosecond. Letâ€™s discuss how the evolution pipeline works on Netflix.\n\n---\n\nev-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.",
    "explanation": "The context does not mention the use of Kubernetes in the system, making this query irrelevant.",
    "judgement": "Not Applicable"
},
{
    "query": "Is there any mention of using asynchronous network calls instead of synchronous ones?",
    "context": "When you load the front page you see multiple rows of different kinds of movies. Netflix personalizes this data and decides what kind of rows or what kind of movies should be displayed to a specific user. This data is based on the userâ€™s historical data and preferences.\n\nAlso, for that specific user, Netflix performs sorting of the movies and calculates the relevance ranking (for the recommendation) of these movies available on their platform. In Netflix, Apache Spark is used for content recommendations and personalization.\n\nA majority of the machine learning pipelines are run on these large spark clusters. These pipelines are then used to do row selection, sorting, title relevance ranking, and artwork personalization among others.\n\n---\n\nev-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.\n\n---\n\nmysql\n\nAll the read queries are redirected to the read replicas and only the write queries are redirected to the master nodes.\n\nIn the case of a primary master MySQL failure, the secondary master node will take over the primary role, and the route53 (DNS configuration) entry for the database will be changed to this new primary node.\nThis will also redirect the write queries to this new primary master node.  \n4.2. Cassandra\nCassandra is a NoSQL database that can handle large amounts of data and it can also handle heavy writing and reading. When Netflix started acquiring more users, the viewing history data for each member also started increasing. This increases the total number of viewing history data and it becomes challenging for Netflix to handle this massive amount of data.\n\n---\n\nTreat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.",
    "explanation": "The context does not mention the use of asynchronous network calls instead of synchronous ones.",
    "judgement": "No"
},
{
    "query": "Is there any mention of implementing circuit breaker patterns?",
    "context": "Hystrix library is designed to do this job. It helps you to control the interactions between these distributed services by adding latency tolerance and fault tolerance logic. Hystrix does this by isolating points of access between the services, remote system, and 3rd party libraries.",
    "explanation": "The context mentions the use of the Hystrix library, which is a well-known implementation of the circuit breaker pattern. It is used to add latency tolerance and fault tolerance logic by isolating points of access between services.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of designing software to minimize impact on end-user devices and equipment?",
    "context": "Treat Servers as Stateless:\nTo understand this concept think of your servers like a herd of cows and you care about how many gallons of milk you get every day.\nIf one day you notice that youâ€™re getting less milk from a cow then you just need to replace that cow (producing less milk) with another cow.\nYou donâ€™t need to be dependent on a specific cow to get the required amount of milk. We can relate the above example to our application.\nThe idea is to design the service in such a way that if one of the endpoints is giving the error or if itâ€™s not serving the request in a timely fashion then you can switch to another server and get your work done.\n3. Low Level Design of Netflix System Design\n3.1. How Does Netflix Onboard a Movie/Video?\n\n---\n\nAn outbound filter is used for zipping the content, calculating the metrics, or adding/removing custom headers. After that, the response is sent back to the Netty server and then it is received by the client.\nAdvantages of using ZUUL:\n\n---\n\nThese replicas require a lot of transcoding and preprocessing. Netflix breaks the original video into different smaller chunks and using parallel workers in AWS it converts these chunks into different formats (like mp4, 3gp, etc) across different resolutions (like 4k, 1080p, and more). After transcoding, once we have multiple copies of the files for the same movie, these files are transferred to each and every Open Connect server which is placed in different locations across the world.\n\nBelow is the step by step process of how Netflix ensures optimal streaming quality:\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nNow, there is a good chance that the other person will also have a similar pattern and he/she will do the same thing that the first person has done.\nContent-based filtering:\nThe idea is to filter those videos which are similar to the video a user has liked before.\nContent-based filtering is highly dependent on the information from the products such as movie title, release year, actors, the genre.\nSo to implement this filtering itâ€™s important to know the information describing each item and some sort of user profile describing what the user likes is also desirable.\n4. Database Design of Netflix System Design\nNetflix uses two different databases i.e. MySQL(RDBMS) and Cassandra(NoSQL) for different purposes.",
    "explanation": "The context does not mention any efforts or strategies focused on minimizing the impact of software on end-user devices and equipment.",
    "judgement": "No"
},
{
    "query": "Is there any mention of choosing server regions that are closest to users?",
    "context": "So if youâ€™re trying to play the video sitting in North America, the video will be served from the nearest open connect (or server) instead of the original server (faster response from the nearest server).",
    "explanation": "The context mentions serving video from the nearest open connect server instead of the original server, indicating a strategy to choose server regions closest to users for faster response.",
    "judgement": "Yes"
},
{
    "query": "Is there any mention of terminating TLS at the border gateway?",
    "context": "v-cache\n\nTo solve this problem Netflix has built its own custom caching layer called EV cache. EV cache is based on Memcached and it is actually a wrapper around Memcached.\n\nNetflix has deployed a lot of clusters in a number of AWS EC2 instances and these clusters have so many nodes of Memcached and they also have cache clients.\n\n---\n\n3.2. How Netflix balance the high traffic load\n1. Elastic Load Balancer\nelastic-load-balancer\n\nELB in Netflix is responsible for routing the traffic to front-end services. ELB performs a two-tier load-balancing scheme where the load is balanced over zones first and then instances (servers).\n\n---\n\nIt includes a lot of powerful and flexible toolkits to display, monitor, and analyze the result.\nChukwe collects the events from different parts of the system and from Chukwe you can do monitoring and analysis or you can use the dashboard to view the events.\nChukwe writes the event in the Hadoop file sequence format (S3). After that Big Data team processes these S3 Hadoop files and writes Hive in Parquet data format.\nThis process is called batch processing which basically scans the whole data at the hourly or daily frequency.\nTo upload online events to EMR/S3, Chukwa also provide traffic to Kafka (the main gate in real-time data processing).\n\n---\n\nNetflix uses Kafka and Apache Chukwe to ingest the data which is produced in a different part of the system. Netflix provides almost 500B data events that consume 1.3 PB/day and 8 million events that consume 24 GB/Second during peak time. These events include information like:\n\nError logs\nUI activities\nPerformance events\nVideo viewing activities\nTroubleshooting and diagnostic events.\nApache Chukwe is an open-source data collection system for collecting logs or events from a distributed system. It is built on top of HDFS and Map-reduce framework. It comes with Hadoopâ€™s scalability and robustness features.\n\n---\n\nmysql\n\nAll the read queries are redirected to the read replicas and only the write queries are redirected to the master nodes.\n\nIn the case of a primary master MySQL failure, the secondary master node will take over the primary role, and the route53 (DNS configuration) entry for the database will be changed to this new primary node.\nThis will also redirect the write queries to this new primary master node.\n4.2. Cassandra\nCassandra is a NoSQL database that can handle large amounts of data and it can also handle heavy writing and reading. When Netflix started acquiring more users, the viewing history data for each member also started increasing. This increases the total number of viewing history data and it becomes challenging for Netflix to handle this massive amount of data.",
    "explanation": "The context does not mention terminating TLS at the border gateway or any related concept.",
    "judgement": "No"
}
]

